{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ccc08d-bb73-49ce-b7c1-febfba611a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1412\u001b[0m\n\u001b[1;32m   1410\u001b[0m parser \u001b[38;5;241m=\u001b[39m build_parser()\n\u001b[1;32m   1411\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m-> 1412\u001b[0m main(args)\n",
      "\u001b[0;31mTypeError\u001b[0m: main() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "(Peptides) — Unified Proof-of-Concept Peptide Design Platform (One-File)\n",
    "\n",
    "\n",
    "This SINGLE script merges the 4 notebooks/scripts you provided into one coherent,\n",
    "non-redundant, end-to-end peptide discovery \"platform demo\".\n",
    "\n",
    "Core capabilities included (combined, de-duplicated):\n",
    "A) Closed-loop Design → Predict → Propose → \"Measure\" → Learn (active learning)\n",
    "B) Multi-objective optimization (Affinity + Stability + Solubility)\n",
    "C) RandomForest ensemble surrogates with uncertainty (mean/std across bootstrap models)\n",
    "D) Bayesian-style acquisition (UCB / EI / Pareto-weighted UCB \"EHVI-lite\")\n",
    "E) True 3-objective Pareto front detection + 2D and 3D visualization\n",
    "F) Diversity constraints (Levenshtein or fast proxy)\n",
    "G) Manufacturability / liability gates + penalties\n",
    "   - Deamidation hotspots: NG, NS, QG, QN\n",
    "   - Asp-Pro cleavage: DP\n",
    "   - Oxidation risk: high M/W\n",
    "   - Cys risk: excessive C\n",
    "   - Extreme net charge\n",
    "   - Aggregation proxy (hydrophobic/aromatic)\n",
    "H) CRO-ready outputs\n",
    "   - Proposed candidates per round (CSV)\n",
    "   - Synthesis order sheet (CSV)\n",
    "   - 96-well plate map (CSV + PNG)\n",
    "   - Screening decision report (Top50 CSV)\n",
    "I) Optional REAL assay ingestion + QC gates (if assay CSVs provided)\n",
    "   - Ingests dose–response or single-point data\n",
    "   - Replicate CV flags\n",
    "   - Plate QC Z' factor (if controls provided)\n",
    "   - 4-parameter logistic (4PL) fit WITHOUT SciPy (grid + refinement)\n",
    "\n",
    "Typical usage (SIMULATED closed-loop, uses internal oracle):\n",
    "  python peptide_platform_unified.py --outdir out_iso_peptides --seed 7 --rounds 8 --acq UCB\n",
    "\n",
    "Fast diversity (skip Levenshtein):\n",
    "  python peptide_platform_unified.py --fast\n",
    "\n",
    "Disable 2-mer features for speed:\n",
    "  python peptide_platform_unified.py --no_kmers\n",
    "\n",
    "Use EI acquisition:\n",
    "  python peptide_platform_unified.py --acq EI\n",
    "\n",
    "REAL assay ingestion + propose NEXT batch (platform-like):\n",
    "  python peptide_platform_unified.py --outdir out_iso_assay --seed 7 --rounds 1 \\\n",
    "      --assay_csv assay_primary.csv --stability_csv assay_stability.csv --solubility_csv assay_solubility.csv \\\n",
    "      --acq EI\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Default mode is SIMULATION (oracle) unless --assay_csv is provided.\n",
    "- If --assay_csv is provided, the script will ingest assays to build measured tables,\n",
    "  train surrogates, then propose the next batch with CRO artifacts.\n",
    "\n",
    "Dependencies:\n",
    "  numpy, pandas, matplotlib, scikit-learn\n",
    "No GPU required.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For 3D plots (matplotlib)\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "\n",
   
    "# 0) Constants + helpers\n",
  
    "\n",
    "AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# Kyte–Doolittle hydropathy index\n",
    "HYDROPATHY = {\n",
    "    \"A\": 1.8, \"C\": 2.5, \"D\": -3.5, \"E\": -3.5, \"F\": 2.8,\n",
    "    \"G\": -0.4, \"H\": -3.2, \"I\": 4.5, \"K\": -3.9, \"L\": 3.8,\n",
    "    \"M\": 1.9, \"N\": -3.5, \"P\": -1.6, \"Q\": -3.5, \"R\": -4.5,\n",
    "    \"S\": -0.8, \"T\": -0.7, \"V\": 4.2, \"W\": -0.9, \"Y\": -1.3\n",
    "}\n",
    "\n",
    "# Simplified side-chain charge at phys pH\n",
    "CHARGE = {\"D\": -1, \"E\": -1, \"K\": +1, \"R\": +1, \"H\": +0.1}\n",
    "\n",
    "AROMATIC = set(\"FWY\")\n",
    "POLAR = set(\"STNQH\")\n",
    "HYDROPHOBIC = set(\"AILMVFWY\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def safe_mkdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "\n",
    "def clamp(x: float, lo: float, hi: float) -> float:\n",
    "    return float(np.clip(x, lo, hi))\n",
    "\n",
    "\n",
    "def save_plot(fig: plt.Figure, outpath: str) -> None:\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=220)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
  
    "# 1) Sequence generation + operators\n",

    "\n",
    "def random_peptide(length: int) -> str:\n",
    "    return \"\".join(random.choice(AMINO_ACIDS) for _ in range(length))\n",
    "\n",
    "\n",
    "def generate_library(n: int, length_range: Tuple[int, int]) -> List[str]:\n",
    "    return [random_peptide(random.randint(*length_range)) for _ in range(n)]\n",
    "\n",
    "\n",
    "def mutate_peptide(seq: str, n_mutations: int = 1) -> str:\n",
    "    seq_list = list(seq)\n",
    "    if len(seq_list) == 0:\n",
    "        return seq\n",
    "    idxs = np.random.choice(len(seq_list), size=min(n_mutations, len(seq_list)), replace=False)\n",
    "    for i in idxs:\n",
    "        original = seq_list[i]\n",
    "        choices = [aa for aa in AMINO_ACIDS if aa != original]\n",
    "        seq_list[i] = random.choice(choices)\n",
    "    return \"\".join(seq_list)\n",
    "\n",
    "\n",
    "def crossover(a: str, b: str) -> str:\n",
    "    if len(a) != len(b) or len(a) < 6:\n",
    "        return a\n",
    "    cut = random.randint(2, len(a) - 3)\n",
    "    return a[:cut] + b[cut:]\n",
    "\n",
    "\n",
    "def diversify_candidates(cands: List[str], max_keep: int = 50000) -> List[str]:\n",
    "    unique = list(dict.fromkeys(cands))  # order preserving\n",
    "    return unique[:max_keep]\n",
    "\n",
    "\n",

    "# 2) Diversity constraints\n",

    "\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    if a == b:\n",
    "        return 0\n",
    "    la, lb = len(a), len(b)\n",
    "    if la == 0:\n",
    "        return lb\n",
    "    if lb == 0:\n",
    "        return la\n",
    "    if la < lb:\n",
    "        a, b = b, a\n",
    "        la, lb = lb, la\n",
    "\n",
    "    prev = list(range(lb + 1))\n",
    "    for i in range(1, la + 1):\n",
    "        cur = [i] + [0] * lb\n",
    "        ca = a[i - 1]\n",
    "        for j in range(1, lb + 1):\n",
    "            cb = b[j - 1]\n",
    "            ins = cur[j - 1] + 1\n",
    "            dele = prev[j] + 1\n",
    "            sub = prev[j - 1] + (0 if ca == cb else 1)\n",
    "            cur[j] = min(ins, dele, sub)\n",
    "        prev = cur\n",
    "    return prev[-1]\n",
    "\n",
    "\n",
    "def greedy_diverse_selection(\n",
    "    df: pd.DataFrame,\n",
    "    k: int,\n",
    "    min_lev_dist: int = 3,\n",
    "    fast: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Greedy: iterate by score order, accept candidate if far enough from all chosen.\n",
    "\n",
    "    fast=True uses a cheap proxy:\n",
    "      - if same length, mismatches < min_lev_dist fail\n",
    "      - if different length, require abs(length diff) >= 2 (else fail)\n",
    "    \"\"\"\n",
    "    chosen_rows = []\n",
    "    chosen_seqs: List[str] = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        s = row[\"sequence\"]\n",
    "        if not chosen_seqs:\n",
    "            chosen_rows.append(row)\n",
    "            chosen_seqs.append(s)\n",
    "        else:\n",
    "            ok = True\n",
    "            for cs in chosen_seqs:\n",
    "                if fast:\n",
    "                    if len(s) != len(cs):\n",
    "                        if abs(len(s) - len(cs)) < 2:\n",
    "                            ok = False\n",
    "                            break\n",
    "                    else:\n",
    "                        mism = sum(1 for x, y in zip(s, cs) if x != y)\n",
    "                        if mism < min_lev_dist:\n",
    "                            ok = False\n",
    "                            break\n",
    "                else:\n",
    "                    if levenshtein(s, cs) < min_lev_dist:\n",
    "                        ok = False\n",
    "                        break\n",
    "            if ok:\n",
    "                chosen_rows.append(row)\n",
    "                chosen_seqs.append(s)\n",
    "\n",
    "        if len(chosen_rows) >= k:\n",
    "            break\n",
    "\n",
    "    if not chosen_rows:\n",
    "        return df.head(k).copy()\n",
    "    return pd.DataFrame(chosen_rows).reset_index(drop=True)\n",
    "\n",
    "\n",

    "# 3) Feature engineering (sequence -> numeric)\n",

    "\n",
    "def net_charge(seq: str) -> float:\n",
    "    return float(sum(CHARGE.get(a, 0.0) for a in seq))\n",
    "\n",
    "\n",
    "def aromatic_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in AROMATIC) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def hydrophobic_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in HYDROPHOBIC) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def polar_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in POLAR) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def aa_composition(seq: str) -> np.ndarray:\n",
    "    counts = np.array([seq.count(a) for a in AMINO_ACIDS], dtype=float)\n",
    "    return counts / max(1, len(seq))\n",
    "\n",
    "\n",
    "def sliding_hydrophobic_windows(seq: str, win: int = 5) -> float:\n",
    "    if len(seq) < win:\n",
    "        return 0.0\n",
    "    vals = []\n",
    "    for i in range(len(seq) - win + 1):\n",
    "        window = seq[i:i + win]\n",
    "        vals.append(np.mean([HYDROPATHY[x] for x in window]))\n",
    "    return float(max(vals))\n",
    "\n",
    "\n",
    "def helix_propensity_proxy(seq: str) -> float:\n",
    "    \"\"\"\n",
    "    Rough helix propensity proxy:\n",
    "    - A, L, E, K, M favor helix\n",
    "    - P, G disrupt helix\n",
    "    \"\"\"\n",
    "    helix_pos = set(\"ALEKM\")\n",
    "    good = sum(1 for a in seq if a in helix_pos) / max(1, len(seq))\n",
    "    bad = (seq.count(\"P\") + seq.count(\"G\")) / max(1, len(seq))\n",
    "    return float(good - 1.2 * bad)\n",
    "\n",
    "\n",
    "def hydrophobic_moment_proxy(seq: str) -> float:\n",
    "    \"\"\"\n",
    "    Not a true helical moment, but a proxy:\n",
    "    hydropathy autocorrelation at lag=3,4 (helical-ish periodicity).\n",
    "    \"\"\"\n",
    "    x = np.array([HYDROPATHY[a] for a in seq], dtype=float)\n",
    "    if len(x) < 6:\n",
    "        return 0.0\n",
    "\n",
    "    def corr(lag: int) -> float:\n",
    "        a = x[:-lag]\n",
    "        b = x[lag:]\n",
    "        return float(np.mean(a * b))\n",
    "\n",
    "    return 0.5 * (corr(3) + corr(4))\n",
    "\n",
    "\n",
    "def motif_counts(seq: str) -> Dict[str, int]:\n",
    "    motifs = {\"RGD\": 0, \"FYF\": 0, \"WxxW\": 0, \"KxxK\": 0}\n",
    "    motifs[\"RGD\"] = sum(1 for i in range(len(seq) - 2) if seq[i:i + 3] == \"RGD\")\n",
    "    motifs[\"FYF\"] = sum(1 for i in range(len(seq) - 2) if seq[i:i + 3] == \"FYF\")\n",
    "    motifs[\"WxxW\"] = sum(1 for i in range(len(seq) - 3) if seq[i] == \"W\" and seq[i + 3] == \"W\")\n",
    "    motifs[\"KxxK\"] = sum(1 for i in range(len(seq) - 3) if seq[i] == \"K\" and seq[i + 3] == \"K\")\n",
    "    return motifs\n",
    "\n",
    "\n",
    "def physchem_features(seq: str) -> np.ndarray:\n",
    "    L = len(seq)\n",
    "    hydro = np.array([HYDROPATHY[a] for a in seq], dtype=float)\n",
    "    q = net_charge(seq)\n",
    "    motifs = motif_counts(seq)\n",
    "\n",
    "    feats = [\n",
    "        float(L),\n",
    "        float(np.mean(hydro)),\n",
    "        float(np.std(hydro)),\n",
    "        float(np.max(hydro)),\n",
    "        float(np.min(hydro)),\n",
    "        float(sliding_hydrophobic_windows(seq, win=5)),\n",
    "        float(q),\n",
    "        float(abs(q)),\n",
    "        float(aromatic_fraction(seq)),\n",
    "        float(hydrophobic_fraction(seq)),\n",
    "        float(polar_fraction(seq)),\n",
    "        float(seq.count(\"P\") / max(1, L)),\n",
    "        float(seq.count(\"G\") / max(1, L)),\n",
    "        float(seq.count(\"C\") / max(1, L)),\n",
    "        float(seq.count(\"M\") / max(1, L)),\n",
    "        float(seq.count(\"W\") / max(1, L)),\n",
    "        float(helix_propensity_proxy(seq)),\n",
    "        float(hydrophobic_moment_proxy(seq)),\n",
    "        float(motifs[\"RGD\"]),\n",
    "        float(motifs[\"FYF\"]),\n",
    "        float(motifs[\"WxxW\"]),\n",
    "        float(motifs[\"KxxK\"]),\n",
    "    ]\n",
    "    return np.array(feats, dtype=float)\n",
    "\n",
    "\n",
    "def kmer_counts(seq: str, k: int = 2, vocab: Optional[List[str]] = None) -> np.ndarray:\n",
    "    if vocab is None:\n",
    "        vocab = [a + b for a in AMINO_ACIDS for b in AMINO_ACIDS]\n",
    "    idx = {v: i for i, v in enumerate(vocab)}\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "\n",
    "    if len(seq) < k:\n",
    "        return vec\n",
    "\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        km = seq[i:i + k]\n",
    "        if km in idx:\n",
    "            vec[idx[km]] += 1.0\n",
    "\n",
    "    vec = vec / max(1.0, (len(seq) - k + 1))\n",
    "    return vec\n",
    "\n",
    "\n",
    "def featurize(seqs: List[str], use_kmers: bool = True) -> np.ndarray:\n",
    "    vocab2 = [a + b for a in AMINO_ACIDS for b in AMINO_ACIDS] if use_kmers else None\n",
    "    X = []\n",
    "    for s in seqs:\n",
    "        parts = [aa_composition(s), physchem_features(s)]\n",
    "        if use_kmers:\n",
    "            parts.append(kmer_counts(s, k=2, vocab=vocab2))\n",
    "        X.append(np.concatenate(parts))\n",
    "    return np.vstack(X)\n",
    "\n",
    "\n",

    "# 4) Liabilities / manufacturability gates\n",

    "\n",
    "@dataclass\n",
    "class LiabilityConfig:\n",
    "    max_cys: int = 1\n",
    "    max_met: int = 2\n",
    "    max_trp: int = 2\n",
    "    max_abs_charge: float = 4.0\n",
    "    max_hydrophobic_fraction: float = 0.65\n",
    "    max_aromatic_fraction: float = 0.30\n",
    "    disallow_motifs: Tuple[str, ...] = (\"NG\", \"NS\", \"QG\", \"QN\", \"DP\")  # deamidation + Asp-Pro cleavage\n",
    "\n",
    "\n",
    "def liability_flags(seq: str, cfg: LiabilityConfig) -> Dict[str, float]:\n",
    "    flags: Dict[str, float] = {}\n",
    "    flags[\"flag_deamidation_hotspot\"] = float(any(m in seq for m in (\"NG\", \"NS\", \"QG\", \"QN\")))\n",
    "    flags[\"flag_asp_pro_cleavage\"] = float(\"DP\" in seq)\n",
    "    flags[\"flag_oxidation_risk\"] = float(seq.count(\"M\") > cfg.max_met or seq.count(\"W\") > cfg.max_trp)\n",
    "    flags[\"flag_cysteine_risk\"] = float(seq.count(\"C\") > cfg.max_cys)\n",
    "    flags[\"flag_extreme_charge\"] = float(abs(net_charge(seq)) > cfg.max_abs_charge)\n",
    "\n",
    "    hyd = hydrophobic_fraction(seq)\n",
    "    aro = aromatic_fraction(seq)\n",
    "    flags[\"flag_aggregation_risk\"] = float((hyd > cfg.max_hydrophobic_fraction) or (aro > cfg.max_aromatic_fraction))\n",
    "\n",
    "    flags[\"liability_count\"] = float(sum(flags[k] for k in flags if k.startswith(\"flag_\")))\n",
    "    flags[\"length\"] = float(len(seq))\n",
    "    flags[\"net_charge\"] = float(net_charge(seq))\n",
    "    flags[\"hydrophobic_fraction\"] = float(hyd)\n",
    "    flags[\"aromatic_fraction\"] = float(aro)\n",
    "    return flags\n",
    "\n",
    "\n",
    "def liability_score(seq: str, cfg: LiabilityConfig) -> float:\n",
    "    f = liability_flags(seq, cfg)\n",
    "    penalty = (\n",
    "        1.4 * f[\"flag_deamidation_hotspot\"] +\n",
    "        1.2 * f[\"flag_asp_pro_cleavage\"] +\n",
    "        1.0 * f[\"flag_oxidation_risk\"] +\n",
    "        1.2 * f[\"flag_cysteine_risk\"] +\n",
    "        0.9 * f[\"flag_extreme_charge\"] +\n",
    "        1.1 * f[\"flag_aggregation_risk\"]\n",
    "    )\n",
    "    return float(penalty)\n",
    "\n",
    "\n",
    "def pass_liability_gate(seq: str, cfg: LiabilityConfig, max_flags: int = 2) -> bool:\n",
    "    f = liability_flags(seq, cfg)\n",
    "    return int(f[\"liability_count\"]) <= max_flags\n",
    "\n",
    "\n",
 
    "# 5) Oracle (simulated truth) for demo mode\n",

    "\n",
    "@dataclass\n",
    "class OracleWeights:\n",
    "    motif_bonus: float = 2.4\n",
    "    hydrophobic_window_bonus: float = 1.1\n",
    "    length_optimum: int = 14\n",
    "    length_penalty: float = 0.06\n",
    "    sbdd_bonus: float = 0.9\n",
    "\n",
    "\n",
    "def oracle(seq: str, w: OracleWeights = OracleWeights(), noise: float = 0.15) -> Dict[str, float]:\n",
    "    motifs = motif_counts(seq)\n",
    "    motif_hits = motifs[\"RGD\"] + motifs[\"FYF\"] + motifs[\"WxxW\"] + motifs[\"KxxK\"]\n",
    "\n",
    "    L = len(seq)\n",
    "    hydro_win = sliding_hydrophobic_windows(seq, win=5)\n",
    "    qabs = abs(net_charge(seq))\n",
    "    cys = seq.count(\"C\")\n",
    "    pro = seq.count(\"P\")\n",
    "    aro = aromatic_fraction(seq)\n",
    "    hyd = hydrophobic_fraction(seq)\n",
    "\n",
    "    sbdd = helix_propensity_proxy(seq) + 0.35 * hydrophobic_moment_proxy(seq)\n",
    "\n",
    "    affinity = (\n",
    "        1.0\n",
    "        + w.motif_bonus * motif_hits\n",
    "        + w.hydrophobic_window_bonus * max(0.0, hydro_win)\n",
    "        + w.sbdd_bonus * sbdd\n",
    "        - 0.30 * (hyd > 0.62) * (hyd - 0.62) * 10\n",
    "        - w.length_penalty * (L - w.length_optimum) ** 2\n",
    "    )\n",
    "\n",
    "    stability = (\n",
    "        1.0\n",
    "        + 0.9 * (1.0 - qabs / max(1.0, L / 2))\n",
    "        - 0.70 * (pro / max(1, L))\n",
    "        - 1.00 * (cys / max(1, L))\n",
    "        + 0.35 * (0.25 <= hyd <= 0.55)\n",
    "        + 0.25 * sbdd\n",
    "    )\n",
    "\n",
    "    solubility = (\n",
    "        1.0\n",
    "        + 0.95 * polar_fraction(seq)\n",
    "        - 0.95 * aro\n",
    "        - 0.85 * max(0.0, hyd - 0.55) * 2.0\n",
    "        + 0.15 * (qabs > 1.0)\n",
    "        - 0.2 * max(0.0, sbdd - 0.75)\n",
    "    )\n",
    "\n",
    "    # Assay-like noise\n",
    "    affinity += np.random.normal(0, noise)\n",
    "    stability += np.random.normal(0, noise)\n",
    "    solubility += np.random.normal(0, noise)\n",
    "\n",
    "    affinity = clamp(affinity, -2.0, 14.0)\n",
    "    stability = clamp(stability, -1.0, 5.0)\n",
    "    solubility = clamp(solubility, -1.0, 5.0)\n",
    "\n",
    "    return {\"affinity\": affinity, \"stability\": stability, \"solubility\": solubility}\n",
    "\n",
    "\n",
    "def measure_sequences_oracle(seqs: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for s in seqs:\n",
    "        out = oracle(s)\n",
    "        rows.append({\"sequence\": s, **out})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",

    "# 6) Pareto front (maximize all objectives)\n",

    "\n",
    "def pareto_front(df: pd.DataFrame, objectives: List[str]) -> pd.Series:\n",
    "    vals = df[objectives].values\n",
    "    n = vals.shape[0]\n",
    "    is_pareto = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not is_pareto[i]:\n",
    "            continue\n",
    "        dominates = np.all(vals >= vals[i], axis=1) & np.any(vals > vals[i], axis=1)\n",
    "        if np.any(dominates):\n",
    "            is_pareto[i] = False\n",
    "    return pd.Series(is_pareto, index=df.index)\n",
    "\n",
    "\n",

    "# 7) ML surrogates + uncertainty\n",

    "\n",
    "def fit_model(X: np.ndarray, y: np.ndarray, seed: int) -> Pipeline:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=450,\n",
    "        random_state=seed,\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"rf\", rf)])\n",
    "    pipe.fit(X, y)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def train_ensemble(X: np.ndarray, y: np.ndarray, seed: int, n_models: int = 5) -> List[Pipeline]:\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        rng = np.random.default_rng(seed + i)\n",
    "        idx = rng.choice(len(X), size=len(X), replace=True)\n",
    "        models.append(fit_model(X[idx], y[idx], seed=seed + i))\n",
    "    return models\n",
    "\n",
    "\n",
    "def ensemble_predict(models: List[Pipeline], X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    preds = np.vstack([m.predict(X) for m in models])\n",
    "    return preds.mean(axis=0), preds.std(axis=0)\n",
    "\n",
    "\n",
    
    "# 8) Acquisition functions (BO-like)\n",

    "\n",
    "def ucb(mu: np.ndarray, sigma: np.ndarray, beta: float = 1.8) -> np.ndarray:\n",
    "    return mu + beta * sigma\n",
    "\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, sigma: np.ndarray, best: float, xi: float = 0.02) -> np.ndarray:\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "    z = (mu - best - xi) / sigma\n",
    "    Phi = 0.5 * (1.0 + np.erf(z / np.sqrt(2)))\n",
    "    phi = (1.0 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * z**2)\n",
    "    ei = (mu - best - xi) * Phi + sigma * phi\n",
    "    return np.maximum(ei, 0.0)\n",
    "\n",
    "\n",
    "def multiobjective_acq(\n",
    "    mu: Dict[str, np.ndarray],\n",
    "    sd: Dict[str, np.ndarray],\n",
    "    measured_df: pd.DataFrame,\n",
    "    acq: str,\n",
    "    weights: Dict[str, float]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multiobjective acquisition on a composite score:\n",
    "      comp = w1*aff + w2*stab + w3*sol\n",
    "    Supports:\n",
    "      - UCB\n",
    "      - EI\n",
    "      - EHVI (lightweight Pareto-weighted UCB proxy)\n",
    "    \"\"\"\n",
    "    comp_mu = np.zeros_like(next(iter(mu.values())))\n",
    "    comp_var = np.zeros_like(next(iter(sd.values())))\n",
    "    for k, w in weights.items():\n",
    "        comp_mu += w * mu[k]\n",
    "        comp_var += (w ** 2) * (sd[k] ** 2)\n",
    "    comp_sd = np.sqrt(comp_var)\n",
    "\n",
    "    acq_u = acq.upper()\n",
    "    if acq_u == \"UCB\":\n",
    "        return ucb(comp_mu, comp_sd, beta=1.8)\n",
    "\n",
    "    if acq_u == \"EI\":\n",
    "        m = measured_df.copy()\n",
    "        m[\"comp\"] = weights[\"affinity\"] * m[\"affinity\"] + weights[\"stability\"] * m[\"stability\"] + weights[\"solubility\"] * m[\"solubility\"]\n",
    "        best = float(m[\"comp\"].max())\n",
    "        return expected_improvement(comp_mu, comp_sd, best=best, xi=0.02)\n",
    "\n",
    "    # EHVI-lite: reward \"balanced\" points and exploration\n",
    "    balance = 0.5 * sigmoid(mu[\"stability\"] - 1.1) + 0.5 * sigmoid(mu[\"solubility\"] - 1.0)\n",
    "    return ucb(comp_mu, comp_sd, beta=1.6) * (0.7 + 0.7 * balance)\n",
    "\n",
    "\n",

    "# 9) Proposal generation (mutations/crossover -> predict -> acquire -> gate -> diverse)\n",

    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    library_size: int = 7000\n",
    "    initial_measured: int = 80\n",
    "    propose_per_round: int = 48\n",
    "    rounds: int = 8\n",
    "    length_min: int = 10\n",
    "    length_max: int = 20\n",
    "    kmers: bool = True\n",
    "    mutations_per_candidate: int = 2\n",
    "    crossover_rate: float = 0.18\n",
    "    min_lev_dist: int = 3\n",
    "    fast: bool = False\n",
    "    acq: str = \"UCB\"\n",
    "    max_liability_flags: int = 2\n",
    "    n_elites: int = 40\n",
    "    n_models: int = 5\n",
    "\n",
    "\n",
    "def expand_pool_from_elites(measured_df: pd.DataFrame, cfg: Config) -> List[str]:\n",
    "    df = measured_df.copy()\n",
    "    df[\"elite_score\"] = 1.0 * df[\"affinity\"] + 0.8 * df[\"stability\"] + 0.6 * df[\"solubility\"]\n",
    "    elites = df.sort_values(\"elite_score\", ascending=False).head(cfg.n_elites)[\"sequence\"].tolist()\n",
    "\n",
    "    props: List[str] = []\n",
    "    for e in elites:\n",
    "        for _ in range(12):\n",
    "            props.append(mutate_peptide(e, n_mutations=cfg.mutations_per_candidate))\n",
    "        if random.random() < cfg.crossover_rate:\n",
    "            partner = random.choice(elites)\n",
    "            props.append(crossover(e, partner))\n",
    "    return diversify_candidates(props, max_keep=50000)\n",
    "\n",
    "\n",
    "def propose_candidates(\n",
    "    measured_df: pd.DataFrame,\n",
    "    pool_seqs: List[str],\n",
    "    cfg: Config,\n",
    "    weights: Dict[str, float],\n",
    "    lcfg: LiabilityConfig\n",
    ") -> pd.DataFrame:\n",
    "    measured_seqs = measured_df[\"sequence\"].tolist()\n",
    "    X_meas = featurize(measured_seqs, use_kmers=cfg.kmers)\n",
    "\n",
    "    # Train ensemble per objective\n",
    "    models: Dict[str, List[Pipeline]] = {}\n",
    "    mu: Dict[str, np.ndarray] = {}\n",
    "    sd: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for obj in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        y = measured_df[obj].values\n",
    "        models[obj] = train_ensemble(\n",
    "            X_meas,\n",
    "            y,\n",
    "            seed=cfg.seed + (hash(obj) % 999),\n",
    "            n_models=cfg.n_models\n",
    "        )\n",
    "\n",
    "    measured_set = set(measured_seqs)\n",
    "    remaining = [s for s in pool_seqs if s not in measured_set]\n",
    "    if len(remaining) == 0:\n",
    "        return pd.DataFrame(columns=[\"sequence\"])\n",
    "\n",
    "    X_pool = featurize(remaining, use_kmers=cfg.kmers)\n",
    "\n",
    "    for obj in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        mu[obj], sd[obj] = ensemble_predict(models[obj], X_pool)\n",
    "\n",
    "    # Acquisition\n",
    "    acq_vals = multiobjective_acq(mu, sd, measured_df, acq=cfg.acq, weights=weights)\n",
    "\n",
    "    # Liability penalty\n",
    "    penalties = np.array([liability_score(s, lcfg) for s in remaining], dtype=float)\n",
    "    penalized_acq = acq_vals - 0.85 * penalties\n",
    "\n",
    "    prop = pd.DataFrame({\n",
    "        \"sequence\": remaining,\n",
    "        \"pred_affinity\": mu[\"affinity\"],\n",
    "        \"pred_stability\": mu[\"stability\"],\n",
    "        \"pred_solubility\": mu[\"solubility\"],\n",
    "        \"sd_affinity\": sd[\"affinity\"],\n",
    "        \"sd_stability\": sd[\"stability\"],\n",
    "        \"sd_solubility\": sd[\"solubility\"],\n",
    "        \"liability_penalty\": penalties,\n",
    "        \"acq_score\": penalized_acq\n",
    "    }).sort_values(\"acq_score\", ascending=False)\n",
    "\n",
    "    # Hard gate\n",
    "    prop[\"pass_gate\"] = prop[\"sequence\"].apply(lambda s: pass_liability_gate(s, lcfg, max_flags=cfg.max_liability_flags))\n",
    "    gated = prop[prop[\"pass_gate\"]].copy()\n",
    "    if len(gated) < cfg.propose_per_round:\n",
    "        gated = prop.copy()\n",
    "\n",
    "    # Diversity select\n",
    "    diverse = greedy_diverse_selection(\n",
    "        gated,\n",
    "        k=cfg.propose_per_round,\n",
    "        min_lev_dist=cfg.min_lev_dist,\n",
    "        fast=cfg.fast\n",
    "    )\n",
    "\n",
    "    # Add liability flags to proposals\n",
    "    flags_df = pd.DataFrame([liability_flags(s, lcfg) for s in diverse[\"sequence\"].tolist()])\n",
    "    out = pd.concat([diverse.reset_index(drop=True), flags_df.reset_index(drop=True)], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
  
    "# 10) CRO artifacts: order sheet + plate map\n",

    "\n",
    "def make_96well_map(seqs: List[str]) -> pd.DataFrame:\n",
    "    rows = list(\"ABCDEFGH\")\n",
    "    cols = list(range(1, 13))\n",
    "    plate = pd.DataFrame(\"\", index=rows, columns=cols)\n",
    "\n",
    "    i = 0\n",
    "    for r in rows:\n",
    "        for c in cols:\n",
    "            if i < len(seqs):\n",
    "                plate.loc[r, c] = f\"Pep_{i+1:03d}\"\n",
    "            i += 1\n",
    "    return plate\n",
    "\n",
    "\n",
    "def save_plate_map_png(plate_df: pd.DataFrame, outpath: str, title: str = \"96-well plate map\") -> None:\n",
    "    fig = plt.figure(figsize=(10.5, 4.8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axis(\"off\")\n",
    "    tbl = ax.table(\n",
    "        cellText=plate_df.values,\n",
    "        rowLabels=plate_df.index,\n",
    "        colLabels=[str(c) for c in plate_df.columns],\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(8)\n",
    "    tbl.scale(1.0, 1.25)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=220)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def build_synthesis_order_sheet(\n",
    "    proposal_df: pd.DataFrame,\n",
    "    outdir: str,\n",
    "    prefix: str,\n",
    "    default_purity: str = \">=95% (HPLC)\",\n",
    "    default_scale: str = \"5 mg\",\n",
    "    default_mods: str = \"None\"\n",
    ") -> pd.DataFrame:\n",
    "    df = proposal_df.copy().reset_index(drop=True)\n",
    "    df.insert(0, \"peptide_id\", [f\"Pep_{i+1:03d}\" for i in range(len(df))])\n",
    "    df[\"purity_spec\"] = default_purity\n",
    "    df[\"synthesis_scale\"] = default_scale\n",
    "    df[\"modifications\"] = default_mods\n",
    "    df[\"notes\"] = df.apply(\n",
    "        lambda r: f\"Liabilities={int(r['liability_count'])}, charge={r['net_charge']:.1f}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    cols = [\n",
    "        \"peptide_id\", \"sequence\", \"length\", \"modifications\",\n",
    "        \"synthesis_scale\", \"purity_spec\",\n",
    "        \"pred_affinity\", \"pred_stability\", \"pred_solubility\",\n",
    "        \"acq_score\",\n",
    "        \"liability_count\",\n",
    "        \"flag_deamidation_hotspot\",\n",
    "        \"flag_asp_pro_cleavage\",\n",
    "        \"flag_oxidation_risk\",\n",
    "        \"flag_cysteine_risk\",\n",
    "        \"flag_extreme_charge\",\n",
    "        \"flag_aggregation_risk\",\n",
    "        \"notes\"\n",
    "    ]\n",
    "    df = df[cols]\n",
    "    df.to_csv(os.path.join(outdir, f\"{prefix}_synthesis_order_sheet.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "\n",
  
    "# 11) Plotting suite (distributions, Pareto 2D/3D, learning curve)\n",

    "\n",
    "def plot_distributions(measured_df: pd.DataFrame, outdir: str) -> None:\n",
    "    for col in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        fig = plt.figure(figsize=(7.8, 5.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.hist(measured_df[col], bins=30, alpha=0.7)\n",
    "        ax.set_xlabel(col.capitalize())\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(f\"Measured distribution: {col}\")\n",
    "        save_plot(fig, os.path.join(outdir, f\"fig_dist_{col}.png\"))\n",
    "\n",
    "\n",
    "def plot_pareto_2d(measured_df: pd.DataFrame, outdir: str, r: int) -> None:\n",
    "    df = measured_df.copy()\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(7.8, 5.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(df[\"stability\"], df[\"affinity\"], alpha=0.5)\n",
    "    ax.scatter(df.loc[df[\"pareto\"], \"stability\"], df.loc[df[\"pareto\"], \"affinity\"], alpha=0.95)\n",
    "    ax.set_xlabel(\"Stability (higher better)\")\n",
    "    ax.set_ylabel(\"Affinity (higher better)\")\n",
    "    ax.set_title(f\"Round {r}: Affinity vs Stability (Pareto highlighted)\")\n",
    "    save_plot(fig, os.path.join(outdir, f\"fig_round{r:02d}_pareto_2d.png\"))\n",
    "\n",
    "\n",
    "def plot_pareto_3d(measured_df: pd.DataFrame, outdir: str, r: int) -> None:\n",
    "    df = measured_df.copy()\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(8.2, 6.0))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(df[\"stability\"], df[\"solubility\"], df[\"affinity\"], alpha=0.45)\n",
    "    ax.scatter(\n",
    "        df.loc[df[\"pareto\"], \"stability\"],\n",
    "        df.loc[df[\"pareto\"], \"solubility\"],\n",
    "        df.loc[df[\"pareto\"], \"affinity\"],\n",
    "        alpha=0.95\n",
    "    )\n",
    "    ax.set_xlabel(\"Stability\")\n",
    "    ax.set_ylabel(\"Solubility\")\n",
    "    ax.set_zlabel(\"Affinity\")\n",
    "    ax.set_title(f\"Round {r}: 3D Pareto landscape\")\n",
    "    save_plot(fig, os.path.join(outdir, f\"fig_round{r:02d}_pareto_3d.png\"))\n",
    "\n",
    "\n",
    "def plot_learning(history_df: pd.DataFrame, outdir: str) -> None:\n",
    "    fig = plt.figure(figsize=(7.8, 5.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history_df[\"n_measured\"], history_df[\"best_affinity\"], marker=\"o\")\n",
    "    ax.plot(history_df[\"n_measured\"], history_df[\"best_composite\"], marker=\"o\")\n",
    "    ax.set_xlabel(\"Measured peptides (cumulative)\")\n",
    "    ax.set_ylabel(\"Best score\")\n",
    "    ax.set_title(\"Closed-loop learning: best affinity and best composite score\")\n",
    "    ax.legend([\"Best affinity\", \"Best composite\"], loc=\"best\")\n",
    "    save_plot(fig, os.path.join(outdir, \"fig_learning_curve.png\"))\n",
    "\n",
    "\n",
  
    "# 12) Screening decision report\n",
 
    "\n",
    "def make_screening_report(measured_df: pd.DataFrame, outdir: str, lcfg: LiabilityConfig, topn: int = 50) -> pd.DataFrame:\n",
    "    df = measured_df.copy()\n",
    "    df[\"composite\"] = 1.0 * df[\"affinity\"] + 0.8 * df[\"stability\"] + 0.6 * df[\"solubility\"]\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    flags_df = pd.DataFrame([liability_flags(s, lcfg) for s in df[\"sequence\"].tolist()])\n",
    "    df = pd.concat([df.reset_index(drop=True), flags_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    rep = df.sort_values([\"composite\", \"pareto\"], ascending=False).head(topn).reset_index(drop=True)\n",
    "    rep.to_csv(os.path.join(outdir, \"screening_report_top50.csv\"), index=False)\n",
    "    return rep\n",
    "\n",
    "\n",
   
    "# 13) REAL assay ingestion + QC (optional)\n",
  
    "\n",
    "def zprime(pos: np.ndarray, neg: np.ndarray) -> float:\n",
    "    pos = np.asarray(pos, dtype=float)\n",
    "    neg = np.asarray(neg, dtype=float)\n",
    "    if len(pos) < 2 or len(neg) < 2:\n",
    "        return float(\"nan\")\n",
    "    mu_p, mu_n = pos.mean(), neg.mean()\n",
    "    sd_p, sd_n = pos.std(ddof=1), neg.std(ddof=1)\n",
    "    denom = abs(mu_p - mu_n) + 1e-12\n",
    "    return float(1.0 - (3.0 * (sd_p + sd_n) / denom))\n",
    "\n",
    "\n",
    "def robust_mean(x: np.ndarray, trim_frac: float = 0.10) -> float:\n",
    "    x = np.sort(np.asarray(x, dtype=float))\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return float(\"nan\")\n",
    "    k = int(math.floor(n * trim_frac))\n",
    "    x2 = x[k:n - k] if n - 2 * k > 0 else x\n",
    "    return float(np.mean(x2))\n",
    "\n",
    "\n",
    "def cv_percent(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < 2:\n",
    "        return float(\"nan\")\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x, ddof=1)\n",
    "    return float(100.0 * sd / (abs(mu) + 1e-12))\n",
    "\n",
    "\n",
    "# 4PL without SciPy\n",
    "\n",
    "def four_pl(x: np.ndarray, bottom: float, top: float, logIC50: float, hill: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    ic50 = 10.0 ** float(logIC50)\n",
    "    return bottom + (top - bottom) / (1.0 + (x / (ic50 + 1e-12)) ** (hill + 1e-12))\n",
    "\n",
    "\n",
    "def fit_4pl_grid(x: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    if len(np.unique(x)) < 4:\n",
    "        return {\n",
    "            \"bottom\": float(np.min(y)),\n",
    "            \"top\": float(np.max(y)),\n",
    "            \"logIC50\": float(np.log10(np.median(x) + 1e-12)),\n",
    "            \"hill\": 1.0,\n",
    "            \"rmse\": float(np.sqrt(np.mean((y - np.mean(y)) ** 2)))\n",
    "        }\n",
    "\n",
    "    # Top/bottom guesses from extreme concentrations\n",
    "    y_low = np.mean(y[x == np.min(x)]) if np.any(x == np.min(x)) else np.max(y)\n",
    "    y_high = np.mean(y[x == np.max(x)]) if np.any(x == np.max(x)) else np.min(y)\n",
    "    bottom0 = float(min(y_low, y_high))\n",
    "    top0 = float(max(y_low, y_high))\n",
    "\n",
    "    logx = np.log10(x + 1e-12)\n",
    "    logIC50_grid = np.linspace(np.min(logx) - 1.0, np.max(logx) + 1.0, 40)\n",
    "    hill_grid = np.linspace(0.6, 2.4, 20)\n",
    "\n",
    "    best = {\"rmse\": float(\"inf\"), \"bottom\": bottom0, \"top\": top0, \"logIC50\": 0.0, \"hill\": 1.0}\n",
    "\n",
    "    for logIC50 in logIC50_grid:\n",
    "        for hill in hill_grid:\n",
    "            pred = four_pl(x, bottom0, top0, logIC50, hill)\n",
    "            rmse = float(np.sqrt(np.mean((y - pred) ** 2)))\n",
    "            if rmse < best[\"rmse\"]:\n",
    "                best = {\"rmse\": rmse, \"bottom\": bottom0, \"top\": top0, \"logIC50\": float(logIC50), \"hill\": float(hill)}\n",
    "\n",
    "    rng = np.random.default_rng(123)\n",
    "    for _ in range(250):\n",
    "        logIC50 = best[\"logIC50\"] + rng.normal(0, 0.12)\n",
    "        hill = clamp(best[\"hill\"] + rng.normal(0, 0.12), 0.4, 4.0)\n",
    "        bottom = best[\"bottom\"] + rng.normal(0, 0.05 * (np.std(y) + 1e-12))\n",
    "        top = best[\"top\"] + rng.normal(0, 0.05 * (np.std(y) + 1e-12))\n",
    "\n",
    "        pred = four_pl(x, bottom, top, logIC50, hill)\n",
    "        rmse = float(np.sqrt(np.mean((y - pred) ** 2)))\n",
    "        if rmse < best[\"rmse\"]:\n",
    "            best = {\"rmse\": rmse, \"bottom\": float(bottom), \"top\": float(top), \"logIC50\": float(logIC50), \"hill\": float(hill)}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def ingest_primary_assay_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Required columns:\n",
    "      peptide_id, concentration_uM, response, replicate\n",
    "    Optional:\n",
    "      plate_id, well, assay_name, timestamp, operator\n",
    "      is_pos_ctrl, is_neg_ctrl\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"peptide_id\", \"concentration_uM\", \"response\", \"replicate\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Primary assay CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df[\"peptide_id\"] = df[\"peptide_id\"].astype(str)\n",
    "    df[\"concentration_uM\"] = df[\"concentration_uM\"].astype(float)\n",
    "    df[\"response\"] = df[\"response\"].astype(float)\n",
    "    df[\"replicate\"] = df[\"replicate\"].astype(int)\n",
    "\n",
    "    for col in [\"plate_id\", \"well\", \"assay_name\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"NA\"\n",
    "\n",
    "    for col in [\"is_pos_ctrl\", \"is_neg_ctrl\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = False\n",
    "        df[col] = df[col].astype(bool)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_primary_assay(primary_raw: pd.DataFrame, max_cv: float = 25.0) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      peptide_summary_df with canonical 'affinity' score\n",
    "      plate_qc_df with Z' factor (if controls exist)\n",
    "    \"\"\"\n",
    "    df = primary_raw.copy()\n",
    "\n",
    "    plate_rows = []\n",
    "    for plate_id, g in df.groupby(\"plate_id\", dropna=False):\n",
    "        pos = g.loc[g[\"is_pos_ctrl\"], \"response\"].values\n",
    "        neg = g.loc[g[\"is_neg_ctrl\"], \"response\"].values\n",
    "        z = zprime(pos, neg) if (len(pos) > 1 and len(neg) > 1) else float(\"nan\")\n",
    "        plate_rows.append({\"plate_id\": plate_id, \"zprime\": z, \"n_pos\": len(pos), \"n_neg\": len(neg)})\n",
    "    plate_qc = pd.DataFrame(plate_rows)\n",
    "\n",
    "    summaries = []\n",
    "    for pep, g in df.groupby(\"peptide_id\"):\n",
    "        x = g[\"concentration_uM\"].values\n",
    "        y = g[\"response\"].values\n",
    "\n",
    "        cv_by_conc = []\n",
    "        for c, gc in g.groupby(\"concentration_uM\"):\n",
    "            if len(gc) >= 2:\n",
    "                cv_by_conc.append(cv_percent(gc[\"response\"].values))\n",
    "        cv_med = float(np.nanmedian(cv_by_conc)) if len(cv_by_conc) else float(\"nan\")\n",
    "        cv_flag = int((not np.isnan(cv_med)) and (cv_med > max_cv))\n",
    "\n",
    "        unique_conc = np.unique(x)\n",
    "        if len(unique_conc) >= 4:\n",
    "            fit = fit_4pl_grid(x, y)\n",
    "            ic50 = 10.0 ** fit[\"logIC50\"]\n",
    "            pic50 = -math.log10(ic50 + 1e-12)  # µM-based pIC50\n",
    "            affinity_score = float(pic50)\n",
    "\n",
    "            summaries.append({\n",
    "                \"peptide_id\": pep,\n",
    "                \"mode\": \"dose_response\",\n",
    "                \"IC50_uM\": float(ic50),\n",
    "                \"pIC50_uM\": float(pic50),\n",
    "                \"affinity\": affinity_score,\n",
    "                \"fit_rmse\": float(fit[\"rmse\"]),\n",
    "                \"hill\": float(fit[\"hill\"]),\n",
    "                \"top\": float(fit[\"top\"]),\n",
    "                \"bottom\": float(fit[\"bottom\"]),\n",
    "                \"replicate_cv_median_percent\": cv_med,\n",
    "                \"flag_high_cv\": cv_flag\n",
    "            })\n",
    "        else:\n",
    "            mean_resp = robust_mean(y, trim_frac=0.10)\n",
    "            affinity_score = float(mean_resp)\n",
    "            summaries.append({\n",
    "                \"peptide_id\": pep,\n",
    "                \"mode\": \"single_point\",\n",
    "                \"IC50_uM\": float(\"nan\"),\n",
    "                \"pIC50_uM\": float(\"nan\"),\n",
    "                \"affinity\": affinity_score,\n",
    "                \"fit_rmse\": float(\"nan\"),\n",
    "                \"hill\": float(\"nan\"),\n",
    "                \"top\": float(\"nan\"),\n",
    "                \"bottom\": float(\"nan\"),\n",
    "                \"replicate_cv_median_percent\": cv_med,\n",
    "                \"flag_high_cv\": cv_flag\n",
    "            })\n",
    "\n",
    "    pep_summary = pd.DataFrame(summaries)\n",
    "    return pep_summary, plate_qc\n",
    "\n",
    "\n",
    "def ingest_scalar_assay(path: str, value_col: str, name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"peptide_id\", value_col, \"replicate\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df[\"peptide_id\"] = df[\"peptide_id\"].astype(str)\n",
    "    df[value_col] = df[value_col].astype(float)\n",
    "    df[\"replicate\"] = df[\"replicate\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_scalar_assay(df: pd.DataFrame, value_col: str, max_cv: float = 25.0) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for pep, g in df.groupby(\"peptide_id\"):\n",
    "        vals = g[value_col].values\n",
    "        m = robust_mean(vals, trim_frac=0.10)\n",
    "        cv = cv_percent(vals) if len(vals) >= 2 else float(\"nan\")\n",
    "        rows.append({\n",
    "            \"peptide_id\": pep,\n",
    "            value_col: float(m),\n",
    "            f\"{value_col}_cv_percent\": float(cv),\n",
    "            f\"flag_{value_col}_high_cv\": int((not np.isnan(cv)) and (cv > max_cv))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def build_measured_table_from_assays(\n",
    "    proposal_table: pd.DataFrame,\n",
    "    primary_summary: pd.DataFrame,\n",
    "    stability_summary: Optional[pd.DataFrame] = None,\n",
    "    solubility_summary: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Canonical measured table for platform loop:\n",
    "      sequence, affinity, stability, solubility\n",
    "\n",
    "    proposal_table must include:\n",
    "      peptide_id, sequence\n",
    "    \"\"\"\n",
    "    df = proposal_table.copy()\n",
    "\n",
    "    if \"peptide_id\" not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.insert(0, \"peptide_id\", [f\"Pep_{i+1:03d}\" for i in range(len(df))])\n",
    "\n",
    "    merged = df.merge(primary_summary[[\"peptide_id\", \"affinity\"]], on=\"peptide_id\", how=\"left\")\n",
    "\n",
    "    if stability_summary is not None and \"stability_score\" in stability_summary.columns:\n",
    "        merged = merged.merge(stability_summary[[\"peptide_id\", \"stability_score\"]], on=\"peptide_id\", how=\"left\")\n",
    "        merged = merged.rename(columns={\"stability_score\": \"stability\"})\n",
    "    else:\n",
    "        merged[\"stability\"] = 1.0\n",
    "\n",
    "    if solubility_summary is not None and \"solubility_score\" in solubility_summary.columns:\n",
    "        merged = merged.merge(solubility_summary[[\"peptide_id\", \"solubility_score\"]], on=\"peptide_id\", how=\"left\")\n",
    "        merged = merged.rename(columns={\"solubility_score\": \"solubility\"})\n",
    "    else:\n",
    "        merged[\"solubility\"] = 1.0\n",
    "\n",
    "    # Remove rows missing affinity (not measured)\n",
    "    merged = merged.dropna(subset=[\"affinity\"]).copy()\n",
    "\n",
    "    # Ensure numeric\n",
    "    merged[\"affinity\"] = merged[\"affinity\"].astype(float)\n",
    "    merged[\"stability\"] = merged[\"stability\"].astype(float)\n",
    "    merged[\"solubility\"] = merged[\"solubility\"].astype(float)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
   
    "# 14) Main\n",
   
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    ap = argparse.ArgumentParser(description=\"Unified peptide platform PoC (simulation + industry artifacts + optional assay ingestion)\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"out_iso_peptides\")\n",
    "    ap.add_argument(\"--seed\", type=int, default=7)\n",
    "\n",
    "    ap.add_argument(\"--library_size\", type=int, default=7000)\n",
    "    ap.add_argument(\"--initial_measured\", type=int, default=80)\n",
    "    ap.add_argument(\"--propose_per_round\", type=int, default=48)\n",
    "    ap.add_argument(\"--rounds\", type=int, default=8)\n",
    "    ap.add_argument(\"--length_min\", type=int, default=10)\n",
    "    ap.add_argument(\"--length_max\", type=int, default=20)\n",
    "\n",
    "    ap.add_argument(\"--no_kmers\", action=\"store_true\", help=\"Disable 2-mer features (faster)\")\n",
    "    ap.add_argument(\"--fast\", action=\"store_true\", help=\"Fast diversity proxy (skip Levenshtein)\")\n",
    "    ap.add_argument(\"--min_lev_dist\", type=int, default=3)\n",
    "\n",
    "    ap.add_argument(\"--acq\", type=str, default=\"UCB\", choices=[\"UCB\", \"EI\", \"EHVI\"])\n",
    "    ap.add_argument(\"--max_liability_flags\", type=int, default=2)\n",
    "\n",
    "    # Assay ingestion mode\n",
    "    ap.add_argument(\"--assay_csv\", type=str, default=None, help=\"Primary assay CSV (dose-response or single-point)\")\n",
    "    ap.add_argument(\"--stability_csv\", type=str, default=None, help=\"Optional stability assay CSV\")\n",
    "    ap.add_argument(\"--solubility_csv\", type=str, default=None, help=\"Optional solubility assay CSV\")\n",
    "\n",
    "    return ap.parse_args()\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"out\")\n",
    "    p.add_argument(\"--seed\", type=int, default=7)\n",
    "    # add your other args here...\n",
    "    args, unknown = p.parse_known_args()  # ✅ ignores -f kernel.json\n",
    "    return args\n",
    "    set_seed(args.seed)\n",
    "    safe_mkdir(args.outdir)\n",
    "\n",
    "    cfg = Config(\n",
    "        seed=args.seed,\n",
    "        library_size=args.library_size,\n",
    "        initial_measured=args.initial_measured,\n",
    "        propose_per_round=args.propose_per_round,\n",
    "        rounds=args.rounds,\n",
    "        length_min=args.length_min,\n",
    "        length_max=args.length_max,\n",
    "        kmers=not args.no_kmers,\n",
    "        fast=args.fast,\n",
    "        min_lev_dist=args.min_lev_dist,\n",
    "        acq=args.acq,\n",
    "        max_liability_flags=args.max_liability_flags\n",
    "    )\n",
    "\n",
    "    weights = {\"affinity\": 1.0, \"stability\": 0.8, \"solubility\": 0.6}\n",
    "    lcfg = LiabilityConfig()\n",
    "\n",
    "    # Build base library\n",
    "    base = generate_library(cfg.library_size, (cfg.length_min, cfg.length_max))\n",
    "    base = diversify_candidates(base, max_keep=cfg.library_size)\n",
    "\n",

    "    # MODE 1: REAL assay ingestion (if assay_csv provided)\n",

    "    if args.assay_csv is not None:\n",
    "        # Ingest primary assay\n",
    "        primary_raw = ingest_primary_assay_csv(args.assay_csv)\n",
    "        primary_summary, plate_qc = summarize_primary_assay(primary_raw, max_cv=25.0)\n",
    "        plate_qc.to_csv(os.path.join(args.outdir, \"assay_plate_qc.csv\"), index=False)\n",
    "        primary_summary.to_csv(os.path.join(args.outdir, \"assay_primary_summary.csv\"), index=False)\n",
    "\n",
    "        stability_summary = None\n",
    "        if args.stability_csv is not None:\n",
    "            st_raw = ingest_scalar_assay(args.stability_csv, value_col=\"stability_score\", name=\"Stability assay\")\n",
    "            stability_summary = summarize_scalar_assay(st_raw, value_col=\"stability_score\", max_cv=25.0)\n",
    "            stability_summary.to_csv(os.path.join(args.outdir, \"assay_stability_summary.csv\"), index=False)\n",
    "\n",
    "        solubility_summary = None\n",
    "        if args.solubility_csv is not None:\n",
    "            so_raw = ingest_scalar_assay(args.solubility_csv, value_col=\"solubility_score\", name=\"Solubility assay\")\n",
    "            solubility_summary = summarize_scalar_assay(so_raw, value_col=\"solubility_score\", max_cv=25.0)\n",
    "            solubility_summary.to_csv(os.path.join(args.outdir, \"assay_solubility_summary.csv\"), index=False)\n",
    "\n",
    "        # We need sequences to map peptide_id -> sequence.\n",
    "        # For a realistic demo, we create a \"proposal table\" for mapping.\n",
    "        # If your assay_csv already has sequences, you can merge externally.\n",
    "        # Here: we create a best-effort mapping by generating a random sequence per peptide_id.\n",
    "        # In production: the order sheet (peptide_id, sequence) is the authoritative mapping.\n",
    "        unique_ids = sorted(primary_summary[\"peptide_id\"].unique().tolist())\n",
    "        mapping = pd.DataFrame({\n",
    "            \"peptide_id\": unique_ids,\n",
    "            \"sequence\": [random_peptide(random.randint(cfg.length_min, cfg.length_max)) for _ in unique_ids]\n",
    "        })\n",
    "        mapping.to_csv(os.path.join(args.outdir, \"assay_peptide_id_to_sequence_mapping_PLACEHOLDER.csv\"), index=False)\n",
    "\n",
    "        measured_df = build_measured_table_from_assays(\n",
    "            proposal_table=mapping,\n",
    "            primary_summary=primary_summary,\n",
    "            stability_summary=stability_summary,\n",
    "            solubility_summary=solubility_summary\n",
    "        )\n",
    "        measured_df[\"round\"] = 0\n",
    "        measured_df.to_csv(os.path.join(args.outdir, \"measured_table_from_assays_round00.csv\"), index=False)\n",
    "\n",
    "        # Plots + report\n",
    "        if len(measured_df) >= 5:\n",
    "            plot_distributions(measured_df, args.outdir)\n",
    "            plot_pareto_2d(measured_df, args.outdir, r=0)\n",
    "            plot_pareto_3d(measured_df, args.outdir, r=0)\n",
    "\n",
    "        # Propose NEXT batch from base pool\n",
    "        pool = base.copy()\n",
    "        # Expand pool slightly (mutation from current best sequences)\n",
    "        if len(measured_df) >= 10:\n",
    "            pool_add = expand_pool_from_elites(measured_df, cfg)\n",
    "            pool = diversify_candidates(pool + pool_add, max_keep=50000)\n",
    "\n",
    "        proposed = propose_candidates(measured_df, pool, cfg, weights, lcfg)\n",
    "        proposed.to_csv(os.path.join(args.outdir, \"proposed_candidates_next_batch.csv\"), index=False)\n",
    "\n",
    "        # CRO artifacts for the next batch\n",
    "        order = build_synthesis_order_sheet(proposed, args.outdir, prefix=\"next_batch\")\n",
    "        plate = make_96well_map(order[\"sequence\"].tolist())\n",
    "        plate.to_csv(os.path.join(args.outdir, \"next_batch_plate_map_96well.csv\"))\n",
    "        save_plate_map_png(plate, os.path.join(args.outdir, \"next_batch_plate_map_96well.png\"), title=\"Next batch assay plate map (96-well)\")\n",
    "\n",
    "        # Decision report on measured data\n",
    "        report = make_screening_report(measured_df, args.outdir, lcfg, topn=50)\n",
    "\n",
    "        summary = {\n",
    "            \"mode\": \"assay_ingestion\",\n",
    "            \"n_measured_from_assays\": int(len(measured_df)),\n",
    "            \"best_affinity\": float(measured_df[\"affinity\"].max()) if len(measured_df) else float(\"nan\"),\n",
    "            \"best_composite\": float((1.0 * measured_df[\"affinity\"] + 0.8 * measured_df[\"stability\"] + 0.6 * measured_df[\"solubility\"]).max())\n",
    "            if len(measured_df) else float(\"nan\"),\n",
    "            \"outdir\": os.path.abspath(args.outdir),\n",
    "            \"artifacts\": {\n",
    "                \"assay_plate_qc\": \"assay_plate_qc.csv\",\n",
    "                \"assay_primary_summary\": \"assay_primary_summary.csv\",\n",
    "                \"proposed_next_batch\": \"proposed_candidates_next_batch.csv\",\n",
    "                \"order_sheet_next_batch\": \"next_batch_synthesis_order_sheet.csv\",\n",
    "                \"plate_map_next_batch_csv\": \"next_batch_plate_map_96well.csv\",\n",
    "                \"plate_map_next_batch_png\": \"next_batch_plate_map_96well.png\",\n",
    "                \"screening_report_top50\": \"screening_report_top50.csv\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(args.outdir, \"run_summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(\"\\n=== Unified peptide platform complete (ASSAY INGESTION MODE) ===\")\n",
    "        print(\"Output directory:\", summary[\"outdir\"])\n",
    "        print(\"Proposed next batch:\", os.path.join(summary[\"outdir\"], \"proposed_candidates_next_batch.csv\"))\n",
    "        print(\"Order sheet:\", os.path.join(summary[\"outdir\"], \"next_batch_synthesis_order_sheet.csv\"))\n",
    "        print(\"Decision report:\", os.path.join(summary[\"outdir\"], \"screening_report_top50.csv\"))\n",
    "        return\n",
    "\n",

    "    # MODE 2: SIMULATED CLOSED LOOP (oracle)\n",

    "\n",
    "    # Initial measured set\n",
    "    init = random.sample(base, k=min(cfg.initial_measured, len(base)))\n",
    "    measured_df = measure_sequences_oracle(init)\n",
    "    measured_df[\"round\"] = 0\n",
    "    measured_df.to_csv(os.path.join(args.outdir, \"measured_table_round00.csv\"), index=False)\n",
    "\n",
    "    # Initial plots\n",
    "    plot_distributions(measured_df, args.outdir)\n",
    "    plot_pareto_2d(measured_df, args.outdir, r=0)\n",
    "    plot_pareto_3d(measured_df, args.outdir, r=0)\n",
    "\n",
    "    pool = base.copy()\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for r in range(1, cfg.rounds + 1):\n",
    "        # Expand pool from elites\n",
    "        pool_add = expand_pool_from_elites(measured_df, cfg)\n",
    "        pool = diversify_candidates(pool + pool_add, max_keep=50000)\n",
    "\n",
    "        # Propose candidates with constraints + diversity\n",
    "        proposed = propose_candidates(measured_df, pool, cfg, weights, lcfg)\n",
    "        proposed.to_csv(os.path.join(args.outdir, f\"proposed_candidates_round{r:02d}.csv\"), index=False)\n",
    "\n",
    "        # CRO artifacts for this batch\n",
    "        order = build_synthesis_order_sheet(proposed, args.outdir, prefix=f\"round{r:02d}\")\n",
    "        plate = make_96well_map(order[\"sequence\"].tolist())\n",
    "        plate.to_csv(os.path.join(args.outdir, f\"round{r:02d}_plate_map_96well.csv\"))\n",
    "        save_plate_map_png(plate, os.path.join(args.outdir, f\"round{r:02d}_plate_map_96well.png\"),\n",
    "                           title=f\"Round {r} assay plate map (96-well)\")\n",
    "\n",
    "        # Measure using oracle (replace with real assays in production)\n",
    "        new_measured = measure_sequences_oracle(proposed[\"sequence\"].tolist())\n",
    "        new_measured[\"round\"] = r\n",
    "\n",
    "        measured_df = pd.concat([measured_df, new_measured], ignore_index=True)\n",
    "        measured_df.to_csv(os.path.join(args.outdir, f\"measured_table_round{r:02d}.csv\"), index=False)\n",
    "\n",
    "        # Track best\n",
    "        measured_df[\"composite\"] = (\n",
    "            weights[\"affinity\"] * measured_df[\"affinity\"] +\n",
    "            weights[\"stability\"] * measured_df[\"stability\"] +\n",
    "            weights[\"solubility\"] * measured_df[\"solubility\"]\n",
    "        )\n",
    "        best_aff = float(measured_df[\"affinity\"].max())\n",
    "        best_comp = float(measured_df[\"composite\"].max())\n",
    "\n",
    "        history.append({\n",
    "            \"round\": float(r),\n",
    "            \"n_measured\": float(len(measured_df)),\n",
    "            \"best_affinity\": best_aff,\n",
    "            \"best_composite\": best_comp\n",
    "        })\n",
    "\n",
    "        # Plots\n",
    "        plot_pareto_2d(measured_df, args.outdir, r=r)\n",
    "        plot_pareto_3d(measured_df, args.outdir, r=r)\n",
    "\n",
    "        print(f\"[Round {r}] measured={len(measured_df)} best_aff={best_aff:.3f} best_comp={best_comp:.3f}  acq={cfg.acq}\")\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(os.path.join(args.outdir, \"learning_history.csv\"), index=False)\n",
    "    plot_learning(history_df, args.outdir)\n",
    "\n",
    "    # Final decision report\n",
    "    report = make_screening_report(measured_df, args.outdir, lcfg, topn=50)\n",
    "\n",
    "    summary = {\n",
    "        \"mode\": \"simulation_oracle\",\n",
    "        \"total_measured\": int(len(measured_df)),\n",
    "        \"best_affinity\": float(measured_df[\"affinity\"].max()),\n",
    "        \"best_composite\": float(measured_df[\"composite\"].max()),\n",
    "        \"top_sequence\": str(report.loc[0, \"sequence\"]),\n",
    "        \"top_affinity\": float(report.loc[0, \"affinity\"]),\n",
    "        \"top_stability\": float(report.loc[0, \"stability\"]),\n",
    "        \"top_solubility\": float(report.loc[0, \"solubility\"]),\n",
    "        \"acquisition\": cfg.acq,\n",
    "        \"min_lev_dist\": int(cfg.min_lev_dist),\n",
    "        \"fast_diversity\": bool(cfg.fast),\n",
    "        \"max_liability_flags\": int(cfg.max_liability_flags),\n",
    "        \"outdir\": os.path.abspath(args.outdir),\n",
    "        \"artifacts\": {\n",
    "            \"screening_report_top50\": \"screening_report_top50.csv\",\n",
    "            \"learning_history\": \"learning_history.csv\",\n",
    "            \"fig_learning_curve\": \"fig_learning_curve.png\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(args.outdir, \"run_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Unified peptide platform complete (SIMULATION MODE) ===\")\n",
    "    print(\"Output directory:\", summary[\"outdir\"])\n",
    "    print(\"Top candidate:\", summary[\"top_sequence\"])\n",
    "    print(\"Affinity/Stability/Solubility:\", summary[\"top_affinity\"], summary[\"top_stability\"], summary[\"top_solubility\"])\n",
    "    print(\"Saved decision report:\", os.path.join(summary[\"outdir\"], \"screening_report_top50.csv\"))\n",
    "    print(\"Saved run summary:\", os.path.join(summary[\"outdir\"], \"run_summary.json\"))\n",
    "\n",
    "\n",
    "def build_parser():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Full assay planning pipeline\")\n",
    "    parser.add_argument(\"--batch_csv\", type=str, default=\"next_experimental_batch.csv\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"assay_plan_full\")\n",
    "    parser.add_argument(\"--plate_format\", type=str, default=\"96\", choices=[\"96\", \"384\"])\n",
    "    parser.add_argument(\"--reps\", type=int, default=3)\n",
    "    parser.add_argument(\"--conditions\", type=str, default=\"pH7.4,pH6.5\")\n",
    "    parser.add_argument(\"--make_plan_only\", action=\"store_true\")\n",
    "    parser.add_argument(\"--results_csv\", type=str, default=None)\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = build_parser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490a5d85-7a74-4928-bb38-2bb1d6781b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 1] measured=128 best_aff=7.637 best_comp=9.981  acq=UCB\n",
      "[Round 2] measured=176 best_aff=8.002 best_comp=10.261  acq=UCB\n",
      "[Round 3] measured=224 best_aff=8.534 best_comp=10.932  acq=UCB\n",
      "[Round 4] measured=272 best_aff=8.910 best_comp=11.576  acq=UCB\n",
      "[Round 5] measured=320 best_aff=9.228 best_comp=11.664  acq=UCB\n",
      "[Round 6] measured=368 best_aff=9.441 best_comp=12.021  acq=UCB\n",
      "[Round 7] measured=416 best_aff=9.537 best_comp=12.021  acq=UCB\n",
      "[Round 8] measured=464 best_aff=9.933 best_comp=12.314  acq=UCB\n",
      "\n",
      "=== Unified peptide platform complete (SIMULATION MODE) ===\n",
      "Output directory: /Users/petalc01/Isomorphic A Closed-Loop, Multi-Objective Peptide Discovery Platform/out_iso_peptides\n",
      "Top candidate: LVVILILLSKWKDWNHRE\n",
      "Affinity/Stability/Solubility: 9.724070170406915 2.4785372775793384 1.0114125476869518\n",
      "Saved decision report: /Users/petalc01/Isomorphic A Closed-Loop, Multi-Objective Peptide Discovery Platform/out_iso_peptides/screening_report_top50.csv\n",
      "Saved run summary: /Users/petalc01/Isomorphic A Closed-Loop, Multi-Objective Peptide Discovery Platform/out_iso_peptides/run_summary.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "(Peptides) — Unified Proof-of-Concept Peptide Design Platform (One-File)\n",

    "\n",
    "This SINGLE script is a coherent, non-redundant, end-to-end peptide discovery \"platform demo\".\n",
    "\n",
    "Core capabilities included:\n",
    "A) Closed-loop Design → Predict → Propose → \"Measure\" → Learn (active learning)\n",
    "B) Multi-objective optimization (Affinity + Stability + Solubility)\n",
    "C) RandomForest ensemble surrogates with uncertainty (mean/std across bootstrap models)\n",
    "D) Bayesian-style acquisition (UCB / EI / Pareto-weighted UCB \"EHVI-lite\")\n",
    "E) True 3-objective Pareto front detection + 2D and 3D visualization\n",
    "F) Diversity constraints (Levenshtein or fast proxy)\n",
    "G) Manufacturability / liability gates + penalties\n",
    "H) CRO-ready outputs:\n",
    "   - Proposed candidates per round (CSV)\n",
    "   - Synthesis order sheet (CSV)\n",
    "   - 96-well plate map (CSV + PNG)\n",
    "   - Screening decision report (Top50 CSV)\n",
    "I) Optional REAL assay ingestion + QC gates (if assay CSVs provided)\n",
    "   - Ingests dose–response or single-point data\n",
    "   - Replicate CV flags\n",
    "   - Plate QC Z' factor (if controls provided)\n",
    "   - 4-parameter logistic (4PL) fit WITHOUT SciPy (grid + refinement)\n",
    "\n",
    "Typical usage (SIMULATED closed-loop, uses internal oracle):\n",
    "  python peptide_platform_unified.py --outdir out_iso_peptides --seed 7 --rounds 8 --acq UCB\n",
    "\n",
    "Fast diversity (skip Levenshtein):\n",
    "  python peptide_platform_unified.py --fast\n",
    "\n",
    "Disable 2-mer features for speed:\n",
    "  python peptide_platform_unified.py --no_kmers\n",
    "\n",
    "Use EI acquisition:\n",
    "  python peptide_platform_unified.py --acq EI\n",
    "\n",
    "REAL assay ingestion + propose NEXT batch:\n",
    "  python peptide_platform_unified.py --outdir out_iso_assay --seed 7 --rounds 1 \\\n",
    "      --assay_csv assay_primary.csv --stability_csv assay_stability.csv --solubility_csv assay_solubility.csv \\\n",
    "      --acq EI\n",
    "\n",
    "Notes\n",

    "- Default mode is SIMULATION (oracle) unless --assay_csv is provided.\n",
    "- If --assay_csv is provided, the script ingests assays to build measured tables,\n",
    "  trains surrogates, then proposes the next batch with CRO artifacts.\n",
    "\n",
    "Dependencies:\n",
    "  numpy, pandas, matplotlib, scikit-learn\n",
    "No GPU required.\n",
    "\n",
    "IMPORTANT FIXES INCLUDED\n",

    "Only ONE parser (build_parser) and it uses parse_known_args() (Jupyter-safe)\n",
    "Only ONE main(args) function, it accepts args properly\n",
    "Clean __main__ entrypoint calling main(args)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For 3D plots (matplotlib)\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "\n",

    "# 0) Constants + helpers\n",

    "\n",
    "AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# Kyte–Doolittle hydropathy index\n",
    "HYDROPATHY = {\n",
    "    \"A\": 1.8, \"C\": 2.5, \"D\": -3.5, \"E\": -3.5, \"F\": 2.8,\n",
    "    \"G\": -0.4, \"H\": -3.2, \"I\": 4.5, \"K\": -3.9, \"L\": 3.8,\n",
    "    \"M\": 1.9, \"N\": -3.5, \"P\": -1.6, \"Q\": -3.5, \"R\": -4.5,\n",
    "    \"S\": -0.8, \"T\": -0.7, \"V\": 4.2, \"W\": -0.9, \"Y\": -1.3\n",
    "}\n",
    "\n",
    "# Simplified side-chain charge at phys pH\n",
    "CHARGE = {\"D\": -1, \"E\": -1, \"K\": +1, \"R\": +1, \"H\": +0.1}\n",
    "\n",
    "AROMATIC = set(\"FWY\")\n",
    "POLAR = set(\"STNQH\")\n",
    "HYDROPHOBIC = set(\"AILMVFWY\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def safe_mkdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "\n",
    "def clamp(x: float, lo: float, hi: float) -> float:\n",
    "    return float(np.clip(x, lo, hi))\n",
    "\n",
    "\n",
    "def save_plot(fig: plt.Figure, outpath: str) -> None:\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=220)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",

    "# 1) Sequence generation + operators\n",
 
    "\n",
    "def random_peptide(length: int) -> str:\n",
    "    return \"\".join(random.choice(AMINO_ACIDS) for _ in range(length))\n",
    "\n",
    "\n",
    "def generate_library(n: int, length_range: Tuple[int, int]) -> List[str]:\n",
    "    return [random_peptide(random.randint(*length_range)) for _ in range(n)]\n",
    "\n",
    "\n",
    "def mutate_peptide(seq: str, n_mutations: int = 1) -> str:\n",
    "    seq_list = list(seq)\n",
    "    if len(seq_list) == 0:\n",
    "        return seq\n",
    "    idxs = np.random.choice(len(seq_list), size=min(n_mutations, len(seq_list)), replace=False)\n",
    "    for i in idxs:\n",
    "        original = seq_list[i]\n",
    "        choices = [aa for aa in AMINO_ACIDS if aa != original]\n",
    "        seq_list[i] = random.choice(choices)\n",
    "    return \"\".join(seq_list)\n",
    "\n",
    "\n",
    "def crossover(a: str, b: str) -> str:\n",
    "    if len(a) != len(b) or len(a) < 6:\n",
    "        return a\n",
    "    cut = random.randint(2, len(a) - 3)\n",
    "    return a[:cut] + b[cut:]\n",
    "\n",
    "\n",
    "def diversify_candidates(cands: List[str], max_keep: int = 50000) -> List[str]:\n",
    "    unique = list(dict.fromkeys(cands))  # order preserving\n",
    "    return unique[:max_keep]\n",
    "\n",
    "\n",

    "# 2) Diversity constraints\n",

    "\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    if a == b:\n",
    "        return 0\n",
    "    la, lb = len(a), len(b)\n",
    "    if la == 0:\n",
    "        return lb\n",
    "    if lb == 0:\n",
    "        return la\n",
    "    if la < lb:\n",
    "        a, b = b, a\n",
    "        la, lb = lb, la\n",
    "\n",
    "    prev = list(range(lb + 1))\n",
    "    for i in range(1, la + 1):\n",
    "        cur = [i] + [0] * lb\n",
    "        ca = a[i - 1]\n",
    "        for j in range(1, lb + 1):\n",
    "            cb = b[j - 1]\n",
    "            ins = cur[j - 1] + 1\n",
    "            dele = prev[j] + 1\n",
    "            sub = prev[j - 1] + (0 if ca == cb else 1)\n",
    "            cur[j] = min(ins, dele, sub)\n",
    "        prev = cur\n",
    "    return prev[-1]\n",
    "\n",
    "\n",
    "def greedy_diverse_selection(\n",
    "    df: pd.DataFrame,\n",
    "    k: int,\n",
    "    min_lev_dist: int = 3,\n",
    "    fast: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Greedy: iterate by score order, accept candidate if far enough from all chosen.\n",
    "\n",
    "    fast=True uses a cheap proxy:\n",
    "      - if same length, mismatches < min_lev_dist fail\n",
    "      - if different length, require abs(length diff) >= 2 (else fail)\n",
    "    \"\"\"\n",
    "    chosen_rows = []\n",
    "    chosen_seqs: List[str] = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        s = row[\"sequence\"]\n",
    "        if not chosen_seqs:\n",
    "            chosen_rows.append(row)\n",
    "            chosen_seqs.append(s)\n",
    "        else:\n",
    "            ok = True\n",
    "            for cs in chosen_seqs:\n",
    "                if fast:\n",
    "                    if len(s) != len(cs):\n",
    "                        if abs(len(s) - len(cs)) < 2:\n",
    "                            ok = False\n",
    "                            break\n",
    "                    else:\n",
    "                        mism = sum(1 for x, y in zip(s, cs) if x != y)\n",
    "                        if mism < min_lev_dist:\n",
    "                            ok = False\n",
    "                            break\n",
    "                else:\n",
    "                    if levenshtein(s, cs) < min_lev_dist:\n",
    "                        ok = False\n",
    "                        break\n",
    "            if ok:\n",
    "                chosen_rows.append(row)\n",
    "                chosen_seqs.append(s)\n",
    "\n",
    "        if len(chosen_rows) >= k:\n",
    "            break\n",
    "\n",
    "    if not chosen_rows:\n",
    "        return df.head(k).copy()\n",
    "    return pd.DataFrame(chosen_rows).reset_index(drop=True)\n",
    "\n",
    "\n",
  
    "# 3) Feature engineering (sequence -> numeric)\n",

    "\n",
    "def net_charge(seq: str) -> float:\n",
    "    return float(sum(CHARGE.get(a, 0.0) for a in seq))\n",
    "\n",
    "\n",
    "def aromatic_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in AROMATIC) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def hydrophobic_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in HYDROPHOBIC) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def polar_fraction(seq: str) -> float:\n",
    "    return float(sum(1 for a in seq if a in POLAR) / max(1, len(seq)))\n",
    "\n",
    "\n",
    "def aa_composition(seq: str) -> np.ndarray:\n",
    "    counts = np.array([seq.count(a) for a in AMINO_ACIDS], dtype=float)\n",
    "    return counts / max(1, len(seq))\n",
    "\n",
    "\n",
    "def sliding_hydrophobic_windows(seq: str, win: int = 5) -> float:\n",
    "    if len(seq) < win:\n",
    "        return 0.0\n",
    "    vals = []\n",
    "    for i in range(len(seq) - win + 1):\n",
    "        window = seq[i:i + win]\n",
    "        vals.append(np.mean([HYDROPATHY[x] for x in window]))\n",
    "    return float(max(vals))\n",
    "\n",
    "\n",
    "def helix_propensity_proxy(seq: str) -> float:\n",
    "    \"\"\"\n",
    "    Rough helix propensity proxy:\n",
    "    - A, L, E, K, M favor helix\n",
    "    - P, G disrupt helix\n",
    "    \"\"\"\n",
    "    helix_pos = set(\"ALEKM\")\n",
    "    good = sum(1 for a in seq if a in helix_pos) / max(1, len(seq))\n",
    "    bad = (seq.count(\"P\") + seq.count(\"G\")) / max(1, len(seq))\n",
    "    return float(good - 1.2 * bad)\n",
    "\n",
    "\n",
    "def hydrophobic_moment_proxy(seq: str) -> float:\n",
    "    \"\"\"\n",
    "    Not a true helical moment, but a proxy:\n",
    "    hydropathy autocorrelation at lag=3,4 (helical-ish periodicity).\n",
    "    \"\"\"\n",
    "    x = np.array([HYDROPATHY[a] for a in seq], dtype=float)\n",
    "    if len(x) < 6:\n",
    "        return 0.0\n",
    "\n",
    "    def corr(lag: int) -> float:\n",
    "        a = x[:-lag]\n",
    "        b = x[lag:]\n",
    "        return float(np.mean(a * b))\n",
    "\n",
    "    return 0.5 * (corr(3) + corr(4))\n",
    "\n",
    "\n",
    "def motif_counts(seq: str) -> Dict[str, int]:\n",
    "    motifs = {\"RGD\": 0, \"FYF\": 0, \"WxxW\": 0, \"KxxK\": 0}\n",
    "    motifs[\"RGD\"] = sum(1 for i in range(len(seq) - 2) if seq[i:i + 3] == \"RGD\")\n",
    "    motifs[\"FYF\"] = sum(1 for i in range(len(seq) - 2) if seq[i:i + 3] == \"FYF\")\n",
    "    motifs[\"WxxW\"] = sum(1 for i in range(len(seq) - 3) if seq[i] == \"W\" and seq[i + 3] == \"W\")\n",
    "    motifs[\"KxxK\"] = sum(1 for i in range(len(seq) - 3) if seq[i] == \"K\" and seq[i + 3] == \"K\")\n",
    "    return motifs\n",
    "\n",
    "\n",
    "def physchem_features(seq: str) -> np.ndarray:\n",
    "    L = len(seq)\n",
    "    hydro = np.array([HYDROPATHY[a] for a in seq], dtype=float)\n",
    "    q = net_charge(seq)\n",
    "    motifs = motif_counts(seq)\n",
    "\n",
    "    feats = [\n",
    "        float(L),\n",
    "        float(np.mean(hydro)),\n",
    "        float(np.std(hydro)),\n",
    "        float(np.max(hydro)),\n",
    "        float(np.min(hydro)),\n",
    "        float(sliding_hydrophobic_windows(seq, win=5)),\n",
    "        float(q),\n",
    "        float(abs(q)),\n",
    "        float(aromatic_fraction(seq)),\n",
    "        float(hydrophobic_fraction(seq)),\n",
    "        float(polar_fraction(seq)),\n",
    "        float(seq.count(\"P\") / max(1, L)),\n",
    "        float(seq.count(\"G\") / max(1, L)),\n",
    "        float(seq.count(\"C\") / max(1, L)),\n",
    "        float(seq.count(\"M\") / max(1, L)),\n",
    "        float(seq.count(\"W\") / max(1, L)),\n",
    "        float(helix_propensity_proxy(seq)),\n",
    "        float(hydrophobic_moment_proxy(seq)),\n",
    "        float(motifs[\"RGD\"]),\n",
    "        float(motifs[\"FYF\"]),\n",
    "        float(motifs[\"WxxW\"]),\n",
    "        float(motifs[\"KxxK\"]),\n",
    "    ]\n",
    "    return np.array(feats, dtype=float)\n",
    "\n",
    "\n",
    "def kmer_counts(seq: str, k: int = 2, vocab: Optional[List[str]] = None) -> np.ndarray:\n",
    "    if vocab is None:\n",
    "        vocab = [a + b for a in AMINO_ACIDS for b in AMINO_ACIDS]\n",
    "    idx = {v: i for i, v in enumerate(vocab)}\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "\n",
    "    if len(seq) < k:\n",
    "        return vec\n",
    "\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        km = seq[i:i + k]\n",
    "        if km in idx:\n",
    "            vec[idx[km]] += 1.0\n",
    "\n",
    "    vec = vec / max(1.0, (len(seq) - k + 1))\n",
    "    return vec\n",
    "\n",
    "\n",
    "def featurize(seqs: List[str], use_kmers: bool = True) -> np.ndarray:\n",
    "    vocab2 = [a + b for a in AMINO_ACIDS for b in AMINO_ACIDS] if use_kmers else None\n",
    "    X = []\n",
    "    for s in seqs:\n",
    "        parts = [aa_composition(s), physchem_features(s)]\n",
    "        if use_kmers:\n",
    "            parts.append(kmer_counts(s, k=2, vocab=vocab2))\n",
    "        X.append(np.concatenate(parts))\n",
    "    return np.vstack(X)\n",
    "\n",
    "\n",

    "# 4) Liabilities / manufacturability gates\n",

    "\n",
    "@dataclass\n",
    "class LiabilityConfig:\n",
    "    max_cys: int = 1\n",
    "    max_met: int = 2\n",
    "    max_trp: int = 2\n",
    "    max_abs_charge: float = 4.0\n",
    "    max_hydrophobic_fraction: float = 0.65\n",
    "    max_aromatic_fraction: float = 0.30\n",
    "    disallow_motifs: Tuple[str, ...] = (\"NG\", \"NS\", \"QG\", \"QN\", \"DP\")\n",
    "\n",
    "\n",
    "def liability_flags(seq: str, cfg: LiabilityConfig) -> Dict[str, float]:\n",
    "    flags: Dict[str, float] = {}\n",
    "    flags[\"flag_deamidation_hotspot\"] = float(any(m in seq for m in (\"NG\", \"NS\", \"QG\", \"QN\")))\n",
    "    flags[\"flag_asp_pro_cleavage\"] = float(\"DP\" in seq)\n",
    "    flags[\"flag_oxidation_risk\"] = float(seq.count(\"M\") > cfg.max_met or seq.count(\"W\") > cfg.max_trp)\n",
    "    flags[\"flag_cysteine_risk\"] = float(seq.count(\"C\") > cfg.max_cys)\n",
    "    flags[\"flag_extreme_charge\"] = float(abs(net_charge(seq)) > cfg.max_abs_charge)\n",
    "\n",
    "    hyd = hydrophobic_fraction(seq)\n",
    "    aro = aromatic_fraction(seq)\n",
    "    flags[\"flag_aggregation_risk\"] = float((hyd > cfg.max_hydrophobic_fraction) or (aro > cfg.max_aromatic_fraction))\n",
    "\n",
    "    flags[\"liability_count\"] = float(sum(flags[k] for k in flags if k.startswith(\"flag_\")))\n",
    "    flags[\"length\"] = float(len(seq))\n",
    "    flags[\"net_charge\"] = float(net_charge(seq))\n",
    "    flags[\"hydrophobic_fraction\"] = float(hyd)\n",
    "    flags[\"aromatic_fraction\"] = float(aro)\n",
    "    return flags\n",
    "\n",
    "\n",
    "def liability_score(seq: str, cfg: LiabilityConfig) -> float:\n",
    "    f = liability_flags(seq, cfg)\n",
    "    penalty = (\n",
    "        1.4 * f[\"flag_deamidation_hotspot\"] +\n",
    "        1.2 * f[\"flag_asp_pro_cleavage\"] +\n",
    "        1.0 * f[\"flag_oxidation_risk\"] +\n",
    "        1.2 * f[\"flag_cysteine_risk\"] +\n",
    "        0.9 * f[\"flag_extreme_charge\"] +\n",
    "        1.1 * f[\"flag_aggregation_risk\"]\n",
    "    )\n",
    "    return float(penalty)\n",
    "\n",
    "\n",
    "def pass_liability_gate(seq: str, cfg: LiabilityConfig, max_flags: int = 2) -> bool:\n",
    "    f = liability_flags(seq, cfg)\n",
    "    return int(f[\"liability_count\"]) <= max_flags\n",
    "\n",
    "\n",

    "# 5) Oracle (simulated truth) for demo mode\n",

    "\n",
    "@dataclass\n",
    "class OracleWeights:\n",
    "    motif_bonus: float = 2.4\n",
    "    hydrophobic_window_bonus: float = 1.1\n",
    "    length_optimum: int = 14\n",
    "    length_penalty: float = 0.06\n",
    "    sbdd_bonus: float = 0.9\n",
    "\n",
    "\n",
    "def oracle(seq: str, w: OracleWeights = OracleWeights(), noise: float = 0.15) -> Dict[str, float]:\n",
    "    motifs = motif_counts(seq)\n",
    "    motif_hits = motifs[\"RGD\"] + motifs[\"FYF\"] + motifs[\"WxxW\"] + motifs[\"KxxK\"]\n",
    "\n",
    "    L = len(seq)\n",
    "    hydro_win = sliding_hydrophobic_windows(seq, win=5)\n",
    "    qabs = abs(net_charge(seq))\n",
    "    cys = seq.count(\"C\")\n",
    "    pro = seq.count(\"P\")\n",
    "    aro = aromatic_fraction(seq)\n",
    "    hyd = hydrophobic_fraction(seq)\n",
    "\n",
    "    sbdd = helix_propensity_proxy(seq) + 0.35 * hydrophobic_moment_proxy(seq)\n",
    "\n",
    "    affinity = (\n",
    "        1.0\n",
    "        + w.motif_bonus * motif_hits\n",
    "        + w.hydrophobic_window_bonus * max(0.0, hydro_win)\n",
    "        + w.sbdd_bonus * sbdd\n",
    "        - 0.30 * (hyd > 0.62) * (hyd - 0.62) * 10\n",
    "        - w.length_penalty * (L - w.length_optimum) ** 2\n",
    "    )\n",
    "\n",
    "    stability = (\n",
    "        1.0\n",
    "        + 0.9 * (1.0 - qabs / max(1.0, L / 2))\n",
    "        - 0.70 * (pro / max(1, L))\n",
    "        - 1.00 * (cys / max(1, L))\n",
    "        + 0.35 * (0.25 <= hyd <= 0.55)\n",
    "        + 0.25 * sbdd\n",
    "    )\n",
    "\n",
    "    solubility = (\n",
    "        1.0\n",
    "        + 0.95 * polar_fraction(seq)\n",
    "        - 0.95 * aro\n",
    "        - 0.85 * max(0.0, hyd - 0.55) * 2.0\n",
    "        + 0.15 * (qabs > 1.0)\n",
    "        - 0.2 * max(0.0, sbdd - 0.75)\n",
    "    )\n",
    "\n",
    "    affinity += np.random.normal(0, noise)\n",
    "    stability += np.random.normal(0, noise)\n",
    "    solubility += np.random.normal(0, noise)\n",
    "\n",
    "    affinity = clamp(affinity, -2.0, 14.0)\n",
    "    stability = clamp(stability, -1.0, 5.0)\n",
    "    solubility = clamp(solubility, -1.0, 5.0)\n",
    "\n",
    "    return {\"affinity\": affinity, \"stability\": stability, \"solubility\": solubility}\n",
    "\n",
    "\n",
    "def measure_sequences_oracle(seqs: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for s in seqs:\n",
    "        out = oracle(s)\n",
    "        rows.append({\"sequence\": s, **out})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",

    "# 6) Pareto front (maximize all objectives)\n",

    "\n",
    "def pareto_front(df: pd.DataFrame, objectives: List[str]) -> pd.Series:\n",
    "    vals = df[objectives].values\n",
    "    n = vals.shape[0]\n",
    "    is_pareto = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not is_pareto[i]:\n",
    "            continue\n",
    "        dominates = np.all(vals >= vals[i], axis=1) & np.any(vals > vals[i], axis=1)\n",
    "        if np.any(dominates):\n",
    "            is_pareto[i] = False\n",
    "    return pd.Series(is_pareto, index=df.index)\n",
    "\n",
    "\n",

    "# 7) ML surrogates + uncertainty\n",

    "\n",
    "def fit_model(X: np.ndarray, y: np.ndarray, seed: int) -> Pipeline:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=450,\n",
    "        random_state=seed,\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"rf\", rf)])\n",
    "    pipe.fit(X, y)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def train_ensemble(X: np.ndarray, y: np.ndarray, seed: int, n_models: int = 5) -> List[Pipeline]:\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        rng = np.random.default_rng(seed + i)\n",
    "        idx = rng.choice(len(X), size=len(X), replace=True)\n",
    "        models.append(fit_model(X[idx], y[idx], seed=seed + i))\n",
    "    return models\n",
    "\n",
    "\n",
    "def ensemble_predict(models: List[Pipeline], X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    preds = np.vstack([m.predict(X) for m in models])\n",
    "    return preds.mean(axis=0), preds.std(axis=0)\n",
    "\n",
    "\n",

    "# 8) Acquisition functions (BO-like)\n",

    "\n",
    "def ucb(mu: np.ndarray, sigma: np.ndarray, beta: float = 1.8) -> np.ndarray:\n",
    "    return mu + beta * sigma\n",
    "\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, sigma: np.ndarray, best: float, xi: float = 0.02) -> np.ndarray:\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "    z = (mu - best - xi) / sigma\n",
    "    Phi = 0.5 * (1.0 + np.erf(z / np.sqrt(2)))\n",
    "    phi = (1.0 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * z**2)\n",
    "    ei = (mu - best - xi) * Phi + sigma * phi\n",
    "    return np.maximum(ei, 0.0)\n",
    "\n",
    "\n",
    "def multiobjective_acq(\n",
    "    mu: Dict[str, np.ndarray],\n",
    "    sd: Dict[str, np.ndarray],\n",
    "    measured_df: pd.DataFrame,\n",
    "    acq: str,\n",
    "    weights: Dict[str, float]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multiobjective acquisition on a composite score:\n",
    "      comp = w1*aff + w2*stab + w3*sol\n",
    "\n",
    "    Supports:\n",
    "      - UCB\n",
    "      - EI\n",
    "      - EHVI (lightweight Pareto-weighted UCB proxy)\n",
    "    \"\"\"\n",
    "    comp_mu = np.zeros_like(next(iter(mu.values())))\n",
    "    comp_var = np.zeros_like(next(iter(sd.values())))\n",
    "\n",
    "    for k, w in weights.items():\n",
    "        comp_mu += w * mu[k]\n",
    "        comp_var += (w ** 2) * (sd[k] ** 2)\n",
    "    comp_sd = np.sqrt(comp_var)\n",
    "\n",
    "    acq_u = acq.upper()\n",
    "    if acq_u == \"UCB\":\n",
    "        return ucb(comp_mu, comp_sd, beta=1.8)\n",
    "\n",
    "    if acq_u == \"EI\":\n",
    "        m = measured_df.copy()\n",
    "        m[\"comp\"] = weights[\"affinity\"] * m[\"affinity\"] + weights[\"stability\"] * m[\"stability\"] + weights[\"solubility\"] * m[\"solubility\"]\n",
    "        best = float(m[\"comp\"].max())\n",
    "        return expected_improvement(comp_mu, comp_sd, best=best, xi=0.02)\n",
    "\n",
    "    # EHVI-lite: reward \"balanced\" points and exploration\n",
    "    balance = 0.5 * sigmoid(mu[\"stability\"] - 1.1) + 0.5 * sigmoid(mu[\"solubility\"] - 1.0)\n",
    "    return ucb(comp_mu, comp_sd, beta=1.6) * (0.7 + 0.7 * balance)\n",
    "\n",
    "\n",

    "# 9) Proposal generation (mutations/crossover -> predict -> acquire -> gate -> diverse)\n",

    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    library_size: int = 7000\n",
    "    initial_measured: int = 80\n",
    "    propose_per_round: int = 48\n",
    "    rounds: int = 8\n",
    "    length_min: int = 10\n",
    "    length_max: int = 20\n",
    "    kmers: bool = True\n",
    "    mutations_per_candidate: int = 2\n",
    "    crossover_rate: float = 0.18\n",
    "    min_lev_dist: int = 3\n",
    "    fast: bool = False\n",
    "    acq: str = \"UCB\"\n",
    "    max_liability_flags: int = 2\n",
    "    n_elites: int = 40\n",
    "    n_models: int = 5\n",
    "\n",
    "\n",
    "def expand_pool_from_elites(measured_df: pd.DataFrame, cfg: Config) -> List[str]:\n",
    "    df = measured_df.copy()\n",
    "    df[\"elite_score\"] = 1.0 * df[\"affinity\"] + 0.8 * df[\"stability\"] + 0.6 * df[\"solubility\"]\n",
    "    elites = df.sort_values(\"elite_score\", ascending=False).head(cfg.n_elites)[\"sequence\"].tolist()\n",
    "\n",
    "    props: List[str] = []\n",
    "    for e in elites:\n",
    "        for _ in range(12):\n",
    "            props.append(mutate_peptide(e, n_mutations=cfg.mutations_per_candidate))\n",
    "        if random.random() < cfg.crossover_rate:\n",
    "            partner = random.choice(elites)\n",
    "            props.append(crossover(e, partner))\n",
    "    return diversify_candidates(props, max_keep=50000)\n",
    "\n",
    "\n",
    "def propose_candidates(\n",
    "    measured_df: pd.DataFrame,\n",
    "    pool_seqs: List[str],\n",
    "    cfg: Config,\n",
    "    weights: Dict[str, float],\n",
    "    lcfg: LiabilityConfig\n",
    ") -> pd.DataFrame:\n",
    "    measured_seqs = measured_df[\"sequence\"].tolist()\n",
    "    X_meas = featurize(measured_seqs, use_kmers=cfg.kmers)\n",
    "\n",
    "    # Train ensemble per objective\n",
    "    models: Dict[str, List[Pipeline]] = {}\n",
    "    mu: Dict[str, np.ndarray] = {}\n",
    "    sd: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for obj in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        y = measured_df[obj].values\n",
    "        models[obj] = train_ensemble(\n",
    "            X_meas,\n",
    "            y,\n",
    "            seed=cfg.seed + (hash(obj) % 999),\n",
    "            n_models=cfg.n_models\n",
    "        )\n",
    "\n",
    "    measured_set = set(measured_seqs)\n",
    "    remaining = [s for s in pool_seqs if s not in measured_set]\n",
    "    if len(remaining) == 0:\n",
    "        return pd.DataFrame(columns=[\"sequence\"])\n",
    "\n",
    "    X_pool = featurize(remaining, use_kmers=cfg.kmers)\n",
    "\n",
    "    for obj in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        mu[obj], sd[obj] = ensemble_predict(models[obj], X_pool)\n",
    "\n",
    "    # Acquisition\n",
    "    acq_vals = multiobjective_acq(mu, sd, measured_df, acq=cfg.acq, weights=weights)\n",
    "\n",
    "    # Liability penalty\n",
    "    penalties = np.array([liability_score(s, lcfg) for s in remaining], dtype=float)\n",
    "    penalized_acq = acq_vals - 0.85 * penalties\n",
    "\n",
    "    prop = pd.DataFrame({\n",
    "        \"sequence\": remaining,\n",
    "        \"pred_affinity\": mu[\"affinity\"],\n",
    "        \"pred_stability\": mu[\"stability\"],\n",
    "        \"pred_solubility\": mu[\"solubility\"],\n",
    "        \"sd_affinity\": sd[\"affinity\"],\n",
    "        \"sd_stability\": sd[\"stability\"],\n",
    "        \"sd_solubility\": sd[\"solubility\"],\n",
    "        \"liability_penalty\": penalties,\n",
    "        \"acq_score\": penalized_acq\n",
    "    }).sort_values(\"acq_score\", ascending=False)\n",
    "\n",
    "    # Hard gate\n",
    "    prop[\"pass_gate\"] = prop[\"sequence\"].apply(lambda s: pass_liability_gate(s, lcfg, max_flags=cfg.max_liability_flags))\n",
    "    gated = prop[prop[\"pass_gate\"]].copy()\n",
    "    if len(gated) < cfg.propose_per_round:\n",
    "        gated = prop.copy()\n",
    "\n",
    "    # Diversity select\n",
    "    diverse = greedy_diverse_selection(\n",
    "        gated,\n",
    "        k=cfg.propose_per_round,\n",
    "        min_lev_dist=cfg.min_lev_dist,\n",
    "        fast=cfg.fast\n",
    "    )\n",
    "\n",
    "    # Add liability flags to proposals\n",
    "    flags_df = pd.DataFrame([liability_flags(s, lcfg) for s in diverse[\"sequence\"].tolist()])\n",
    "    out = pd.concat([diverse.reset_index(drop=True), flags_df.reset_index(drop=True)], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
 
    "# 10) CRO artifacts: order sheet + plate map\n",

    "\n",
    "def make_96well_map(seqs: List[str]) -> pd.DataFrame:\n",
    "    rows = list(\"ABCDEFGH\")\n",
    "    cols = list(range(1, 13))\n",
    "    plate = pd.DataFrame(\"\", index=rows, columns=cols)\n",
    "\n",
    "    i = 0\n",
    "    for r in rows:\n",
    "        for c in cols:\n",
    "            if i < len(seqs):\n",
    "                plate.loc[r, c] = f\"Pep_{i+1:03d}\"\n",
    "            i += 1\n",
    "    return plate\n",
    "\n",
    "\n",
    "def save_plate_map_png(plate_df: pd.DataFrame, outpath: str, title: str = \"96-well plate map\") -> None:\n",
    "    fig = plt.figure(figsize=(10.5, 4.8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axis(\"off\")\n",
    "    tbl = ax.table(\n",
    "        cellText=plate_df.values,\n",
    "        rowLabels=plate_df.index,\n",
    "        colLabels=[str(c) for c in plate_df.columns],\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(8)\n",
    "    tbl.scale(1.0, 1.25)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=220)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def build_synthesis_order_sheet(\n",
    "    proposal_df: pd.DataFrame,\n",
    "    outdir: str,\n",
    "    prefix: str,\n",
    "    default_purity: str = \">=95% (HPLC)\",\n",
    "    default_scale: str = \"5 mg\",\n",
    "    default_mods: str = \"None\"\n",
    ") -> pd.DataFrame:\n",
    "    df = proposal_df.copy().reset_index(drop=True)\n",
    "    df.insert(0, \"peptide_id\", [f\"Pep_{i+1:03d}\" for i in range(len(df))])\n",
    "    df[\"purity_spec\"] = default_purity\n",
    "    df[\"synthesis_scale\"] = default_scale\n",
    "    df[\"modifications\"] = default_mods\n",
    "    df[\"notes\"] = df.apply(\n",
    "        lambda r: f\"Liabilities={int(r['liability_count'])}, charge={r['net_charge']:.1f}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    cols = [\n",
    "        \"peptide_id\", \"sequence\", \"length\", \"modifications\",\n",
    "        \"synthesis_scale\", \"purity_spec\",\n",
    "        \"pred_affinity\", \"pred_stability\", \"pred_solubility\",\n",
    "        \"acq_score\",\n",
    "        \"liability_count\",\n",
    "        \"flag_deamidation_hotspot\",\n",
    "        \"flag_asp_pro_cleavage\",\n",
    "        \"flag_oxidation_risk\",\n",
    "        \"flag_cysteine_risk\",\n",
    "        \"flag_extreme_charge\",\n",
    "        \"flag_aggregation_risk\",\n",
    "        \"notes\"\n",
    "    ]\n",
    "    df = df[cols]\n",
    "    df.to_csv(os.path.join(outdir, f\"{prefix}_synthesis_order_sheet.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "\n",
  
    "# 11) Plotting suite\n",

    "\n",
    "def plot_distributions(measured_df: pd.DataFrame, outdir: str) -> None:\n",
    "    for col in [\"affinity\", \"stability\", \"solubility\"]:\n",
    "        fig = plt.figure(figsize=(7.8, 5.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.hist(measured_df[col], bins=30, alpha=0.7)\n",
    "        ax.set_xlabel(col.capitalize())\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(f\"Measured distribution: {col}\")\n",
    "        save_plot(fig, os.path.join(outdir, f\"fig_dist_{col}.png\"))\n",
    "\n",
    "\n",
    "def plot_pareto_2d(measured_df: pd.DataFrame, outdir: str, r: int) -> None:\n",
    "    df = measured_df.copy()\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(7.8, 5.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(df[\"stability\"], df[\"affinity\"], alpha=0.5)\n",
    "    ax.scatter(df.loc[df[\"pareto\"], \"stability\"], df.loc[df[\"pareto\"], \"affinity\"], alpha=0.95)\n",
    "    ax.set_xlabel(\"Stability (higher better)\")\n",
    "    ax.set_ylabel(\"Affinity (higher better)\")\n",
    "    ax.set_title(f\"Round {r}: Affinity vs Stability (Pareto highlighted)\")\n",
    "    save_plot(fig, os.path.join(outdir, f\"fig_round{r:02d}_pareto_2d.png\"))\n",
    "\n",
    "\n",
    "def plot_pareto_3d(measured_df: pd.DataFrame, outdir: str, r: int) -> None:\n",
    "    df = measured_df.copy()\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(8.2, 6.0))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(df[\"stability\"], df[\"solubility\"], df[\"affinity\"], alpha=0.45)\n",
    "    ax.scatter(\n",
    "        df.loc[df[\"pareto\"], \"stability\"],\n",
    "        df.loc[df[\"pareto\"], \"solubility\"],\n",
    "        df.loc[df[\"pareto\"], \"affinity\"],\n",
    "        alpha=0.95\n",
    "    )\n",
    "    ax.set_xlabel(\"Stability\")\n",
    "    ax.set_ylabel(\"Solubility\")\n",
    "    ax.set_zlabel(\"Affinity\")\n",
    "    ax.set_title(f\"Round {r}: 3D Pareto landscape\")\n",
    "    save_plot(fig, os.path.join(outdir, f\"fig_round{r:02d}_pareto_3d.png\"))\n",
    "\n",
    "\n",
    "def plot_learning(history_df: pd.DataFrame, outdir: str) -> None:\n",
    "    fig = plt.figure(figsize=(7.8, 5.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history_df[\"n_measured\"], history_df[\"best_affinity\"], marker=\"o\")\n",
    "    ax.plot(history_df[\"n_measured\"], history_df[\"best_composite\"], marker=\"o\")\n",
    "    ax.set_xlabel(\"Measured peptides (cumulative)\")\n",
    "    ax.set_ylabel(\"Best score\")\n",
    "    ax.set_title(\"Closed-loop learning: best affinity and best composite score\")\n",
    "    ax.legend([\"Best affinity\", \"Best composite\"], loc=\"best\")\n",
    "    save_plot(fig, os.path.join(outdir, \"fig_learning_curve.png\"))\n",
    "\n",
    "\n",

    "# 12) Screening decision report\n",

    "\n",
    "def make_screening_report(measured_df: pd.DataFrame, outdir: str, lcfg: LiabilityConfig, topn: int = 50) -> pd.DataFrame:\n",
    "    df = measured_df.copy()\n",
    "    df[\"composite\"] = 1.0 * df[\"affinity\"] + 0.8 * df[\"stability\"] + 0.6 * df[\"solubility\"]\n",
    "    df[\"pareto\"] = pareto_front(df, [\"affinity\", \"stability\", \"solubility\"])\n",
    "\n",
    "    flags_df = pd.DataFrame([liability_flags(s, lcfg) for s in df[\"sequence\"].tolist()])\n",
    "    df = pd.concat([df.reset_index(drop=True), flags_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    rep = df.sort_values([\"composite\", \"pareto\"], ascending=False).head(topn).reset_index(drop=True)\n",
    "    rep.to_csv(os.path.join(outdir, \"screening_report_top50.csv\"), index=False)\n",
    "    return rep\n",
    "\n",
    "\n",
  
    "# 13) REAL assay ingestion + QC (optional)\n",

    "\n",
    "def zprime(pos: np.ndarray, neg: np.ndarray) -> float:\n",
    "    pos = np.asarray(pos, dtype=float)\n",
    "    neg = np.asarray(neg, dtype=float)\n",
    "    if len(pos) < 2 or len(neg) < 2:\n",
    "        return float(\"nan\")\n",
    "    mu_p, mu_n = pos.mean(), neg.mean()\n",
    "    sd_p, sd_n = pos.std(ddof=1), neg.std(ddof=1)\n",
    "    denom = abs(mu_p - mu_n) + 1e-12\n",
    "    return float(1.0 - (3.0 * (sd_p + sd_n) / denom))\n",
    "\n",
    "\n",
    "def robust_mean(x: np.ndarray, trim_frac: float = 0.10) -> float:\n",
    "    x = np.sort(np.asarray(x, dtype=float))\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return float(\"nan\")\n",
    "    k = int(math.floor(n * trim_frac))\n",
    "    x2 = x[k:n - k] if n - 2 * k > 0 else x\n",
    "    return float(np.mean(x2))\n",
    "\n",
    "\n",
    "def cv_percent(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < 2:\n",
    "        return float(\"nan\")\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x, ddof=1)\n",
    "    return float(100.0 * sd / (abs(mu) + 1e-12))\n",
    "\n",
    "\n",
    "def four_pl(x: np.ndarray, bottom: float, top: float, logIC50: float, hill: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    ic50 = 10.0 ** float(logIC50)\n",
    "    return bottom + (top - bottom) / (1.0 + (x / (ic50 + 1e-12)) ** (hill + 1e-12))\n",
    "\n",
    "\n",
    "def fit_4pl_grid(x: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    if len(np.unique(x)) < 4:\n",
    "        return {\n",
    "            \"bottom\": float(np.min(y)),\n",
    "            \"top\": float(np.max(y)),\n",
    "            \"logIC50\": float(np.log10(np.median(x) + 1e-12)),\n",
    "            \"hill\": 1.0,\n",
    "            \"rmse\": float(np.sqrt(np.mean((y - np.mean(y)) ** 2)))\n",
    "        }\n",
    "\n",
    "    y_low = np.mean(y[x == np.min(x)]) if np.any(x == np.min(x)) else np.max(y)\n",
    "    y_high = np.mean(y[x == np.max(x)]) if np.any(x == np.max(x)) else np.min(y)\n",
    "    bottom0 = float(min(y_low, y_high))\n",
    "    top0 = float(max(y_low, y_high))\n",
    "\n",
    "    logx = np.log10(x + 1e-12)\n",
    "    logIC50_grid = np.linspace(np.min(logx) - 1.0, np.max(logx) + 1.0, 40)\n",
    "    hill_grid = np.linspace(0.6, 2.4, 20)\n",
    "\n",
    "    best = {\"rmse\": float(\"inf\"), \"bottom\": bottom0, \"top\": top0, \"logIC50\": 0.0, \"hill\": 1.0}\n",
    "\n",
    "    for logIC50 in logIC50_grid:\n",
    "        for hill in hill_grid:\n",
    "            pred = four_pl(x, bottom0, top0, logIC50, hill)\n",
    "            rmse = float(np.sqrt(np.mean((y - pred) ** 2)))\n",
    "            if rmse < best[\"rmse\"]:\n",
    "                best = {\"rmse\": rmse, \"bottom\": bottom0, \"top\": top0, \"logIC50\": float(logIC50), \"hill\": float(hill)}\n",
    "\n",
    "    rng = np.random.default_rng(123)\n",
    "    for _ in range(250):\n",
    "        logIC50 = best[\"logIC50\"] + rng.normal(0, 0.12)\n",
    "        hill = clamp(best[\"hill\"] + rng.normal(0, 0.12), 0.4, 4.0)\n",
    "        bottom = best[\"bottom\"] + rng.normal(0, 0.05 * (np.std(y) + 1e-12))\n",
    "        top = best[\"top\"] + rng.normal(0, 0.05 * (np.std(y) + 1e-12))\n",
    "\n",
    "        pred = four_pl(x, bottom, top, logIC50, hill)\n",
    "        rmse = float(np.sqrt(np.mean((y - pred) ** 2)))\n",
    "        if rmse < best[\"rmse\"]:\n",
    "            best = {\"rmse\": rmse, \"bottom\": float(bottom), \"top\": float(top), \"logIC50\": float(logIC50), \"hill\": float(hill)}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def ingest_primary_assay_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Required columns:\n",
    "      peptide_id, concentration_uM, response, replicate\n",
    "    Optional:\n",
    "      plate_id, well, assay_name, timestamp, operator\n",
    "      is_pos_ctrl, is_neg_ctrl\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"peptide_id\", \"concentration_uM\", \"response\", \"replicate\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Primary assay CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df[\"peptide_id\"] = df[\"peptide_id\"].astype(str)\n",
    "    df[\"concentration_uM\"] = df[\"concentration_uM\"].astype(float)\n",
    "    df[\"response\"] = df[\"response\"].astype(float)\n",
    "    df[\"replicate\"] = df[\"replicate\"].astype(int)\n",
    "\n",
    "    for col in [\"plate_id\", \"well\", \"assay_name\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"NA\"\n",
    "\n",
    "    for col in [\"is_pos_ctrl\", \"is_neg_ctrl\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = False\n",
    "        df[col] = df[col].astype(bool)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_primary_assay(primary_raw: pd.DataFrame, max_cv: float = 25.0) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      peptide_summary_df with canonical 'affinity' score\n",
    "      plate_qc_df with Z' factor (if controls exist)\n",
    "    \"\"\"\n",
    "    df = primary_raw.copy()\n",
    "\n",
    "    plate_rows = []\n",
    "    for plate_id, g in df.groupby(\"plate_id\", dropna=False):\n",
    "        pos = g.loc[g[\"is_pos_ctrl\"], \"response\"].values\n",
    "        neg = g.loc[g[\"is_neg_ctrl\"], \"response\"].values\n",
    "        z = zprime(pos, neg) if (len(pos) > 1 and len(neg) > 1) else float(\"nan\")\n",
    "        plate_rows.append({\"plate_id\": plate_id, \"zprime\": z, \"n_pos\": len(pos), \"n_neg\": len(neg)})\n",
    "    plate_qc = pd.DataFrame(plate_rows)\n",
    "\n",
    "    summaries = []\n",
    "    for pep, g in df.groupby(\"peptide_id\"):\n",
    "        x = g[\"concentration_uM\"].values\n",
    "        y = g[\"response\"].values\n",
    "\n",
    "        cv_by_conc = []\n",
    "        for c, gc in g.groupby(\"concentration_uM\"):\n",
    "            if len(gc) >= 2:\n",
    "                cv_by_conc.append(cv_percent(gc[\"response\"].values))\n",
    "        cv_med = float(np.nanmedian(cv_by_conc)) if len(cv_by_conc) else float(\"nan\")\n",
    "        cv_flag = int((not np.isnan(cv_med)) and (cv_med > max_cv))\n",
    "\n",
    "        unique_conc = np.unique(x)\n",
    "        if len(unique_conc) >= 4:\n",
    "            fit = fit_4pl_grid(x, y)\n",
    "            ic50 = 10.0 ** fit[\"logIC50\"]\n",
    "            pic50 = -math.log10(ic50 + 1e-12)  # µM-based pIC50\n",
    "            affinity_score = float(pic50)\n",
    "\n",
    "            summaries.append({\n",
    "                \"peptide_id\": pep,\n",
    "                \"mode\": \"dose_response\",\n",
    "                \"IC50_uM\": float(ic50),\n",
    "                \"pIC50_uM\": float(pic50),\n",
    "                \"affinity\": affinity_score,\n",
    "                \"fit_rmse\": float(fit[\"rmse\"]),\n",
    "                \"hill\": float(fit[\"hill\"]),\n",
    "                \"top\": float(fit[\"top\"]),\n",
    "                \"bottom\": float(fit[\"bottom\"]),\n",
    "                \"replicate_cv_median_percent\": cv_med,\n",
    "                \"flag_high_cv\": cv_flag\n",
    "            })\n",
    "        else:\n",
    "            mean_resp = robust_mean(y, trim_frac=0.10)\n",
    "            affinity_score = float(mean_resp)\n",
    "            summaries.append({\n",
    "                \"peptide_id\": pep,\n",
    "                \"mode\": \"single_point\",\n",
    "                \"IC50_uM\": float(\"nan\"),\n",
    "                \"pIC50_uM\": float(\"nan\"),\n",
    "                \"affinity\": affinity_score,\n",
    "                \"fit_rmse\": float(\"nan\"),\n",
    "                \"hill\": float(\"nan\"),\n",
    "                \"top\": float(\"nan\"),\n",
    "                \"bottom\": float(\"nan\"),\n",
    "                \"replicate_cv_median_percent\": cv_med,\n",
    "                \"flag_high_cv\": cv_flag\n",
    "            })\n",
    "\n",
    "    pep_summary = pd.DataFrame(summaries)\n",
    "    return pep_summary, plate_qc\n",
    "\n",
    "\n",
    "def ingest_scalar_assay(path: str, value_col: str, name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"peptide_id\", value_col, \"replicate\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df[\"peptide_id\"] = df[\"peptide_id\"].astype(str)\n",
    "    df[value_col] = df[value_col].astype(float)\n",
    "    df[\"replicate\"] = df[\"replicate\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_scalar_assay(df: pd.DataFrame, value_col: str, max_cv: float = 25.0) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for pep, g in df.groupby(\"peptide_id\"):\n",
    "        vals = g[value_col].values\n",
    "        m = robust_mean(vals, trim_frac=0.10)\n",
    "        cv = cv_percent(vals) if len(vals) >= 2 else float(\"nan\")\n",
    "        rows.append({\n",
    "            \"peptide_id\": pep,\n",
    "            value_col: float(m),\n",
    "            f\"{value_col}_cv_percent\": float(cv),\n",
    "            f\"flag_{value_col}_high_cv\": int((not np.isnan(cv)) and (cv > max_cv))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def build_measured_table_from_assays(\n",
    "    proposal_table: pd.DataFrame,\n",
    "    primary_summary: pd.DataFrame,\n",
    "    stability_summary: Optional[pd.DataFrame] = None,\n",
    "    solubility_summary: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Canonical measured table for platform loop:\n",
    "      sequence, affinity, stability, solubility\n",
    "\n",
    "    proposal_table must include:\n",
    "      peptide_id, sequence\n",
    "    \"\"\"\n",
    "    df = proposal_table.copy()\n",
    "\n",
    "    if \"peptide_id\" not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.insert(0, \"peptide_id\", [f\"Pep_{i+1:03d}\" for i in range(len(df))])\n",
    "\n",
    "    merged = df.merge(primary_summary[[\"peptide_id\", \"affinity\"]], on=\"peptide_id\", how=\"left\")\n",
    "\n",
    "    if stability_summary is not None and \"stability_score\" in stability_summary.columns:\n",
    "        merged = merged.merge(stability_summary[[\"peptide_id\", \"stability_score\"]], on=\"peptide_id\", how=\"left\")\n",
    "        merged = merged.rename(columns={\"stability_score\": \"stability\"})\n",
    "    else:\n",
    "        merged[\"stability\"] = 1.0\n",
    "\n",
    "    if solubility_summary is not None and \"solubility_score\" in solubility_summary.columns:\n",
    "        merged = merged.merge(solubility_summary[[\"peptide_id\", \"solubility_score\"]], on=\"peptide_id\", how=\"left\")\n",
    "        merged = merged.rename(columns={\"solubility_score\": \"solubility\"})\n",
    "    else:\n",
    "        merged[\"solubility\"] = 1.0\n",
    "\n",
    "    merged = merged.dropna(subset=[\"affinity\"]).copy()\n",
    "    merged[\"affinity\"] = merged[\"affinity\"].astype(float)\n",
    "    merged[\"stability\"] = merged[\"stability\"].astype(float)\n",
    "    merged[\"solubility\"] = merged[\"solubility\"].astype(float)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
 
    "# 14) CLI parser + main()\n",

    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    ap = argparse.ArgumentParser(\n",
    "        description=\"Unified peptide platform PoC (simulation + CRO artifacts + optional assay ingestion)\"\n",
    "    )\n",
    "\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"out_iso_peptides\")\n",
    "    ap.add_argument(\"--seed\", type=int, default=7)\n",
    "\n",
    "    ap.add_argument(\"--library_size\", type=int, default=7000)\n",
    "    ap.add_argument(\"--initial_measured\", type=int, default=80)\n",
    "    ap.add_argument(\"--propose_per_round\", type=int, default=48)\n",
    "    ap.add_argument(\"--rounds\", type=int, default=8)\n",
    "    ap.add_argument(\"--length_min\", type=int, default=10)\n",
    "    ap.add_argument(\"--length_max\", type=int, default=20)\n",
    "\n",
    "    ap.add_argument(\"--no_kmers\", action=\"store_true\", help=\"Disable 2-mer features (faster)\")\n",
    "    ap.add_argument(\"--fast\", action=\"store_true\", help=\"Fast diversity proxy (skip Levenshtein)\")\n",
    "    ap.add_argument(\"--min_lev_dist\", type=int, default=3)\n",
    "\n",
    "    ap.add_argument(\"--acq\", type=str, default=\"UCB\", choices=[\"UCB\", \"EI\", \"EHVI\"])\n",
    "    ap.add_argument(\"--max_liability_flags\", type=int, default=2)\n",
    "\n",
    "    # Assay ingestion mode\n",
    "    ap.add_argument(\"--assay_csv\", type=str, default=None, help=\"Primary assay CSV (dose-response or single-point)\")\n",
    "    ap.add_argument(\"--stability_csv\", type=str, default=None, help=\"Optional stability assay CSV\")\n",
    "    ap.add_argument(\"--solubility_csv\", type=str, default=None, help=\"Optional solubility assay CSV\")\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "def main(args: Optional[argparse.Namespace] = None) -> None:\n",
    "    # Jupyter-safe argument parsing (ignores hidden -f kernel.json)\n",
    "    if args is None:\n",
    "        parser = build_parser()\n",
    "        args, _ = parser.parse_known_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    safe_mkdir(args.outdir)\n",
    "\n",
    "    cfg = Config(\n",
    "        seed=args.seed,\n",
    "        library_size=args.library_size,\n",
    "        initial_measured=args.initial_measured,\n",
    "        propose_per_round=args.propose_per_round,\n",
    "        rounds=args.rounds,\n",
    "        length_min=args.length_min,\n",
    "        length_max=args.length_max,\n",
    "        kmers=not args.no_kmers,\n",
    "        fast=args.fast,\n",
    "        min_lev_dist=args.min_lev_dist,\n",
    "        acq=args.acq,\n",
    "        max_liability_flags=args.max_liability_flags\n",
    "    )\n",
    "\n",
    "    weights = {\"affinity\": 1.0, \"stability\": 0.8, \"solubility\": 0.6}\n",
    "    lcfg = LiabilityConfig()\n",
    "\n",
    "    # Build base library\n",
    "    base = generate_library(cfg.library_size, (cfg.length_min, cfg.length_max))\n",
    "    base = diversify_candidates(base, max_keep=cfg.library_size)\n",
    "\n",
  
    "    # MODE 1: REAL assay ingestion (if assay_csv provided)\n",

    "    if args.assay_csv is not None:\n",
    "        primary_raw = ingest_primary_assay_csv(args.assay_csv)\n",
    "        primary_summary, plate_qc = summarize_primary_assay(primary_raw, max_cv=25.0)\n",
    "        plate_qc.to_csv(os.path.join(args.outdir, \"assay_plate_qc.csv\"), index=False)\n",
    "        primary_summary.to_csv(os.path.join(args.outdir, \"assay_primary_summary.csv\"), index=False)\n",
    "\n",
    "        stability_summary = None\n",
    "        if args.stability_csv is not None:\n",
    "            st_raw = ingest_scalar_assay(args.stability_csv, value_col=\"stability_score\", name=\"Stability assay\")\n",
    "            stability_summary = summarize_scalar_assay(st_raw, value_col=\"stability_score\", max_cv=25.0)\n",
    "            stability_summary.to_csv(os.path.join(args.outdir, \"assay_stability_summary.csv\"), index=False)\n",
    "\n",
    "        solubility_summary = None\n",
    "        if args.solubility_csv is not None:\n",
    "            so_raw = ingest_scalar_assay(args.solubility_csv, value_col=\"solubility_score\", name=\"Solubility assay\")\n",
    "            solubility_summary = summarize_scalar_assay(so_raw, value_col=\"solubility_score\", max_cv=25.0)\n",
    "            solubility_summary.to_csv(os.path.join(args.outdir, \"assay_solubility_summary.csv\"), index=False)\n",
    "\n",
    "        # Mapping peptide_id -> sequence should come from your order sheet.\n",
    "        # Here we create a placeholder mapping so the platform remains runnable.\n",
    "        unique_ids = sorted(primary_summary[\"peptide_id\"].unique().tolist())\n",
    "        mapping = pd.DataFrame({\n",
    "            \"peptide_id\": unique_ids,\n",
    "            \"sequence\": [random_peptide(random.randint(cfg.length_min, cfg.length_max)) for _ in unique_ids]\n",
    "        })\n",
    "        mapping.to_csv(os.path.join(args.outdir, \"assay_peptide_id_to_sequence_mapping_PLACEHOLDER.csv\"), index=False)\n",
    "\n",
    "        measured_df = build_measured_table_from_assays(\n",
    "            proposal_table=mapping,\n",
    "            primary_summary=primary_summary,\n",
    "            stability_summary=stability_summary,\n",
    "            solubility_summary=solubility_summary\n",
    "        )\n",
    "        measured_df[\"round\"] = 0\n",
    "        measured_df.to_csv(os.path.join(args.outdir, \"measured_table_from_assays_round00.csv\"), index=False)\n",
    "\n",
    "        # Plots + report\n",
    "        if len(measured_df) >= 5:\n",
    "            plot_distributions(measured_df, args.outdir)\n",
    "            plot_pareto_2d(measured_df, args.outdir, r=0)\n",
    "            plot_pareto_3d(measured_df, args.outdir, r=0)\n",
    "\n",
    "        # Propose NEXT batch from base pool\n",
    "        pool = base.copy()\n",
    "        if len(measured_df) >= 10:\n",
    "            pool_add = expand_pool_from_elites(measured_df, cfg)\n",
    "            pool = diversify_candidates(pool + pool_add, max_keep=50000)\n",
    "\n",
    "        proposed = propose_candidates(measured_df, pool, cfg, weights, lcfg)\n",
    "        proposed.to_csv(os.path.join(args.outdir, \"proposed_candidates_next_batch.csv\"), index=False)\n",
    "\n",
    "        order = build_synthesis_order_sheet(proposed, args.outdir, prefix=\"next_batch\")\n",
    "        plate = make_96well_map(order[\"sequence\"].tolist())\n",
    "        plate.to_csv(os.path.join(args.outdir, \"next_batch_plate_map_96well.csv\"))\n",
    "        save_plate_map_png(\n",
    "            plate,\n",
    "            os.path.join(args.outdir, \"next_batch_plate_map_96well.png\"),\n",
    "            title=\"Next batch assay plate map (96-well)\"\n",
    "        )\n",
    "\n",
    "        _ = make_screening_report(measured_df, args.outdir, lcfg, topn=50)\n",
    "\n",
    "        summary = {\n",
    "            \"mode\": \"assay_ingestion\",\n",
    "            \"n_measured_from_assays\": int(len(measured_df)),\n",
    "            \"best_affinity\": float(measured_df[\"affinity\"].max()) if len(measured_df) else float(\"nan\"),\n",
    "            \"best_composite\": float((1.0 * measured_df[\"affinity\"] + 0.8 * measured_df[\"stability\"] + 0.6 * measured_df[\"solubility\"]).max())\n",
    "            if len(measured_df) else float(\"nan\"),\n",
    "            \"outdir\": os.path.abspath(args.outdir),\n",
    "            \"artifacts\": {\n",
    "                \"assay_plate_qc\": \"assay_plate_qc.csv\",\n",
    "                \"assay_primary_summary\": \"assay_primary_summary.csv\",\n",
    "                \"proposed_next_batch\": \"proposed_candidates_next_batch.csv\",\n",
    "                \"order_sheet_next_batch\": \"next_batch_synthesis_order_sheet.csv\",\n",
    "                \"plate_map_next_batch_csv\": \"next_batch_plate_map_96well.csv\",\n",
    "                \"plate_map_next_batch_png\": \"next_batch_plate_map_96well.png\",\n",
    "                \"screening_report_top50\": \"screening_report_top50.csv\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(args.outdir, \"run_summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(\"\\n=== Unified peptide platform complete (ASSAY INGESTION MODE) ===\")\n",
    "        print(\"Output directory:\", summary[\"outdir\"])\n",
    "        print(\"Proposed next batch:\", os.path.join(summary[\"outdir\"], \"proposed_candidates_next_batch.csv\"))\n",
    "        print(\"Order sheet:\", os.path.join(summary[\"outdir\"], \"next_batch_synthesis_order_sheet.csv\"))\n",
    "        print(\"Decision report:\", os.path.join(summary[\"outdir\"], \"screening_report_top50.csv\"))\n",
    "        return\n",
    "\n",
  
    "    # MODE 2: SIMULATED CLOSED LOOP (oracle)\n",

    "\n",
    "    init = random.sample(base, k=min(cfg.initial_measured, len(base)))\n",
    "    measured_df = measure_sequences_oracle(init)\n",
    "    measured_df[\"round\"] = 0\n",
    "    measured_df.to_csv(os.path.join(args.outdir, \"measured_table_round00.csv\"), index=False)\n",
    "\n",
    "    plot_distributions(measured_df, args.outdir)\n",
    "    plot_pareto_2d(measured_df, args.outdir, r=0)\n",
    "    plot_pareto_3d(measured_df, args.outdir, r=0)\n",
    "\n",
    "    pool = base.copy()\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for r in range(1, cfg.rounds + 1):\n",
    "        pool_add = expand_pool_from_elites(measured_df, cfg)\n",
    "        pool = diversify_candidates(pool + pool_add, max_keep=50000)\n",
    "\n",
    "        proposed = propose_candidates(measured_df, pool, cfg, weights, lcfg)\n",
    "        proposed.to_csv(os.path.join(args.outdir, f\"proposed_candidates_round{r:02d}.csv\"), index=False)\n",
    "\n",
    "        order = build_synthesis_order_sheet(proposed, args.outdir, prefix=f\"round{r:02d}\")\n",
    "        plate = make_96well_map(order[\"sequence\"].tolist())\n",
    "        plate.to_csv(os.path.join(args.outdir, f\"round{r:02d}_plate_map_96well.csv\"))\n",
    "        save_plate_map_png(\n",
    "            plate,\n",
    "            os.path.join(args.outdir, f\"round{r:02d}_plate_map_96well.png\"),\n",
    "            title=f\"Round {r} assay plate map (96-well)\"\n",
    "        )\n",
    "\n",
    "        new_measured = measure_sequences_oracle(proposed[\"sequence\"].tolist())\n",
    "        new_measured[\"round\"] = r\n",
    "\n",
    "        measured_df = pd.concat([measured_df, new_measured], ignore_index=True)\n",
    "        measured_df.to_csv(os.path.join(args.outdir, f\"measured_table_round{r:02d}.csv\"), index=False)\n",
    "\n",
    "        measured_df[\"composite\"] = (\n",
    "            weights[\"affinity\"] * measured_df[\"affinity\"] +\n",
    "            weights[\"stability\"] * measured_df[\"stability\"] +\n",
    "            weights[\"solubility\"] * measured_df[\"solubility\"]\n",
    "        )\n",
    "\n",
    "        best_aff = float(measured_df[\"affinity\"].max())\n",
    "        best_comp = float(measured_df[\"composite\"].max())\n",
    "\n",
    "        history.append({\n",
    "            \"round\": float(r),\n",
    "            \"n_measured\": float(len(measured_df)),\n",
    "            \"best_affinity\": best_aff,\n",
    "            \"best_composite\": best_comp\n",
    "        })\n",
    "\n",
    "        plot_pareto_2d(measured_df, args.outdir, r=r)\n",
    "        plot_pareto_3d(measured_df, args.outdir, r=r)\n",
    "\n",
    "        print(f\"[Round {r}] measured={len(measured_df)} best_aff={best_aff:.3f} best_comp={best_comp:.3f}  acq={cfg.acq}\")\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(os.path.join(args.outdir, \"learning_history.csv\"), index=False)\n",
    "    plot_learning(history_df, args.outdir)\n",
    "\n",
    "    report = make_screening_report(measured_df, args.outdir, lcfg, topn=50)\n",
    "\n",
    "    summary = {\n",
    "        \"mode\": \"simulation_oracle\",\n",
    "        \"total_measured\": int(len(measured_df)),\n",
    "        \"best_affinity\": float(measured_df[\"affinity\"].max()),\n",
    "        \"best_composite\": float(measured_df[\"composite\"].max()),\n",
    "        \"top_sequence\": str(report.loc[0, \"sequence\"]),\n",
    "        \"top_affinity\": float(report.loc[0, \"affinity\"]),\n",
    "        \"top_stability\": float(report.loc[0, \"stability\"]),\n",
    "        \"top_solubility\": float(report.loc[0, \"solubility\"]),\n",
    "        \"acquisition\": cfg.acq,\n",
    "        \"min_lev_dist\": int(cfg.min_lev_dist),\n",
    "        \"fast_diversity\": bool(cfg.fast),\n",
    "        \"max_liability_flags\": int(cfg.max_liability_flags),\n",
    "        \"outdir\": os.path.abspath(args.outdir),\n",
    "        \"artifacts\": {\n",
    "            \"screening_report_top50\": \"screening_report_top50.csv\",\n",
    "            \"learning_history\": \"learning_history.csv\",\n",
    "            \"fig_learning_curve\": \"fig_learning_curve.png\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(args.outdir, \"run_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Unified peptide platform complete (SIMULATION MODE) ===\")\n",
    "    print(\"Output directory:\", summary[\"outdir\"])\n",
    "    print(\"Top candidate:\", summary[\"top_sequence\"])\n",
    "    print(\"Affinity/Stability/Solubility:\", summary[\"top_affinity\"], summary[\"top_stability\"], summary[\"top_solubility\"])\n",
    "    print(\"Saved decision report:\", os.path.join(summary[\"outdir\"], \"screening_report_top50.csv\"))\n",
    "    print(\"Saved run summary:\", os.path.join(summary[\"outdir\"], \"run_summary.json\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = build_parser()\n",
    "    args, _ = parser.parse_known_args()  # ignores notebook \"-f kernel.json\"\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242546e8-eab9-408a-8f40-7c1f77af91ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
